{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "PM-OpidNr-2t"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VD6FmmptNgi",
    "outputId": "7ce67015-39d1-45d4-eacf-eddb26355488"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1915bbdd330>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "_IXCcwxLytwu"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') #required for imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "xq-hPW8IzSOg"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The SwiGLU activation function,\n",
    "from \"GLU Variants Improve Transformer\" (Shazeer, 2020).\n",
    "\n",
    "From the paper:\n",
    "'We offer no explanation as to why these architectures seem to work;\n",
    "we attribute their success, as all else, to __divine benevolence__.'\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    The SwiGLU activation function as proposed by Noam Shazeer.\n",
    "\n",
    "    This module implements the SwiGLU function defined as:\n",
    "    FFN_SwiGLU(x, W, V, W2) = (Swish_{1}(xW) ⊙ (xV))W2\n",
    "    where ⊙ denotes the Hadamard product and Swish_{1} is the Swish function with β=1.\n",
    "\n",
    "    Note: The Swish function with β=1 is equivalent to PyTorch's SiLU function.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Input and output dimension.\n",
    "        h_dim (int): Hidden dimension.\n",
    "        bias (bool, optional): If false, additive biases will not be learned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, h_dim, bias=False):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(dim, h_dim, bias=bias)\n",
    "        self.v = nn.Linear(dim, h_dim, bias=bias)\n",
    "        self.w2 = nn.Linear(h_dim, dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w(x)) * self.v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "a-i6Ahw0uV1d"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def get_hankel(n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates a Hankel matrix Z, as defined in Equation (3) of the paper.\n",
    "\n",
    "    This special matrix is used for the spectral filtering in the Spectral\n",
    "    Transform Unit (STU).\n",
    "\n",
    "    Args:\n",
    "        n (int): Size of the square Hankel matrix.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Hankel matrix Z of shape [n, n].\n",
    "    \"\"\"\n",
    "    i = torch.arange(1, n + 1)\n",
    "    s = i[:, None] + i[None, :]\n",
    "    Z = 2.0 / (s**3 - s)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "mlmAFn2_uaft"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def get_top_eigh(\n",
    "    n: int, K: int, use_hankel_L: bool, device: torch.device\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns the top K eigenvalues and eigenvectors of the Hankel matrix Z.\n",
    "\n",
    "    These eigenvalues and eigenvectors are used to construct the spectral\n",
    "    filters for the STU model, as described in Section 3 of the paper.\n",
    "\n",
    "    Args:\n",
    "        n (int): Size of the Hankel matrix.\n",
    "        K (int): Number of top eigenvalues/eigenvectors to return.\n",
    "        use_hankel_L (bool): If True, use the alternative Hankel matrix Z_L.\n",
    "        device (torch.device): Computation device (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]:\n",
    "            - sigma: Top K eigenvalues [K]\n",
    "            - phi: The corresponding eigenvectors [n, K]\n",
    "    \"\"\"\n",
    "    # Z = get_hankel_L(n).to(device) if use_hankel_L else get_hankel(n).to(device)\n",
    "    Z = get_hankel(n).to(device)\n",
    "    sigma, phi = torch.linalg.eigh(Z)\n",
    "    return sigma[-K:], phi[:, -K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "vBUguWnnsI1z"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def shift(u: torch.Tensor, k: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Rolls the time axis forward by k steps to align the input u_{t-k} with u_t.\n",
    "    This effectively removes the last k time steps of the input tensor.\n",
    "\n",
    "    This function implements the time shifting functionality needed for\n",
    "    the autoregressive component in Equation 4 of the STU model (Section 3).\n",
    "\n",
    "    Args:\n",
    "        u (torch.Tensor): An input tensor of shape [bsz, sl, K, d].\n",
    "        k (int): Number of time steps to shift. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Shifted tensor of shape [bsz, sl, K, d].\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return u\n",
    "    shifted = torch.roll(u, shifts=k, dims=1)\n",
    "    shifted[:, :k] = 0\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "0kjpNlHKsD3Q"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def compute_ar_u(M_u: torch.Tensor, u: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the autoregressive component of the STU model with respect to\n",
    "    the input, as described in Equation (4) of Section 3.\n",
    "\n",
    "    This function implements the sum of M^u_i u_{t+1-i} from i=1 to\n",
    "    (more generally) k_u (in the paper, it was up until i=3).\n",
    "\n",
    "    Args:\n",
    "        M_u (torch.Tensor): Input weight matrices of shape (d_out, k_u, d_in)\n",
    "        u (torch.Tensor): Input tensor of shape (bsz, sl, d_in)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Autoregressive component w.r.t. input of shape (bsz, sl, d_out)\n",
    "    \"\"\"\n",
    "    k_u = M_u.shape[1]\n",
    "\n",
    "    # Sum M^u_i \\hat_{u}_{t+1-i} from i=1 to i=k_u\n",
    "    u_shifted = torch.stack([shift(u, i) for i in range(k_u)], dim=1)\n",
    "    ar_u = torch.einsum(\"bksd,dki->bsi\", u_shifted, M_u)\n",
    "\n",
    "    return ar_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "xxwcxaYex3-K"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def compute_ar_y(M_y: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the autoregressive component of the AR-STU model with respect to\n",
    "    the output, as described in Equation (6) of Section 5.\n",
    "\n",
    "    This function implements the sum of M^y_i y_{t-i} from i=1 to i=k_y.\n",
    "\n",
    "    Args:\n",
    "        M_y: Output weight matrices of shape (d_out, k_y, d_out)\n",
    "        y: Predictions (bsz, sl, d_out)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Autoregressive component w.r.t. output of shape (bsz, sl, d_out)\n",
    "    \"\"\"\n",
    "    # k_y = M_y.shape[0]\n",
    "\n",
    "    # # Sum M^y_i \\hat_{y}_{t-i} from i=1 to i=k_y\n",
    "    # y_shifted = torch.stack([shift(y, i + 1) for i in range(k_y)], dim=1)\n",
    "    # ar_y = torch.einsum(\"bksd,kod->bso\", y_shifted, M_y)\n",
    "    # return ar_y\n",
    "    k, d_out, _ = M_y.shape\n",
    "    bsz, sl, _ = y.shape\n",
    "\n",
    "    # Initialize carry buffer and output tensor\n",
    "    carry = torch.zeros((bsz, k, d_out), device=y.device)\n",
    "    ys = torch.zeros((bsz, sl, d_out), device=y.device)\n",
    "\n",
    "    # Process each timestep\n",
    "    for t in range(sl):\n",
    "        # Current input: (bsz, d_out)\n",
    "        x = y[:, t]\n",
    "\n",
    "        # Compute AR component: (bsz, d_out)\n",
    "        output = torch.einsum('dko,bkd->bo', M_y, carry) + x\n",
    "\n",
    "        # Store output\n",
    "        ys[:, t] = output\n",
    "\n",
    "        # Update carry buffer\n",
    "        carry = torch.cat([output.unsqueeze(1), carry[:, :-1]], dim=1)\n",
    "\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "3qBxujdSAyAG"
   },
   "outputs": [],
   "source": [
    "def compute_ar(M_y, M_u, u):\n",
    "    \"\"\"\n",
    "    Computes both autoregressive components of the STU model\n",
    "    as described in Equation (4) of Section 3.\n",
    "\n",
    "    Args:\n",
    "        M_y (torch.Tensor): Input weight matrices of shape (d_out, k_y, d_in)\n",
    "        M_u (torch.Tensor): Input weight matrices of shape (d_out, k_u, d_in)\n",
    "        u (torch.Tensor): Input tensor of shape (B, L, d_in)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Autoregressive component w.r.t. input of shape (B, L, d_out)\n",
    "    \"\"\"\n",
    "    bsz, seq_len, _ = u.shape\n",
    "    D, k_y, _ = M_y.shape\n",
    "    output_list = []  # Store outputs for each timestep\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        terms = []  # Collect all terms for this timestep\n",
    "\n",
    "        # AR component\n",
    "        for i in range(1, k_y + 1):\n",
    "            if t - i >= 0:\n",
    "                M_i = M_y[:, i-1]\n",
    "                y_prev = output_list[t-i]\n",
    "                terms.append(y_prev @ M_i.T)\n",
    "\n",
    "        # Control component\n",
    "        if M_u is not None:\n",
    "            k_u = M_u.shape[1]\n",
    "            for i in range(1, k_u + 1):\n",
    "                if t + 1 - i >= 0 and t + 1 - i < seq_len:\n",
    "                    u_prev = u[:, t+1-i]\n",
    "                    terms.append(u_prev @ M_u[:, i-1].T)\n",
    "\n",
    "        # Sum all terms for this timestep\n",
    "        if terms:\n",
    "            output_list.append(sum(terms))\n",
    "        else:\n",
    "            output_list.append(torch.zeros(bsz, D, device=u.device))\n",
    "\n",
    "    # Stack all timesteps\n",
    "    return torch.stack(output_list, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOQUSSFMq7lZ",
    "outputId": "cdb5f2f4-3eb9-4f3a-8762-6dc05d672b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def compute_ar_y(M_y: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the autoregressive component of the AR-STU model.\n",
    "\n",
    "    Args:\n",
    "        M_y: Output weight matrices of shape (d_out, k_y, d_out)\n",
    "        y: Predictions (bsz, sl, d_out)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Autoregressive component (bsz, sl, d_out)\n",
    "    \"\"\"\n",
    "    k, d_out, _ = M_y.shape\n",
    "    bsz, sl, _ = y.shape\n",
    "\n",
    "    # Initialize carry buffer and output tensor\n",
    "    carry = torch.zeros((bsz, k, d_out), device=y.device)\n",
    "    ys = torch.zeros((bsz, sl, d_out), device=y.device)\n",
    "\n",
    "    # Process each timestep\n",
    "    for t in range(sl):\n",
    "        # Current input: (bsz, d_out)\n",
    "        x = y[:, t]\n",
    "\n",
    "        # Compute AR component: (bsz, d_out)\n",
    "        output = torch.einsum('kdo,bkd->bo', M_y, carry) + x\n",
    "\n",
    "        # Store output\n",
    "        ys[:, t] = output\n",
    "\n",
    "        # Update carry buffer\n",
    "        carry = torch.cat([output.unsqueeze(1), carry[:, :-1]], dim=1)\n",
    "\n",
    "    return ys\n",
    "\n",
    "def test_compute_ar_y():\n",
    "    \"\"\"Test the autoregressive computation\"\"\"\n",
    "    k_y, d_out = 2, 3\n",
    "    bsz, sl = 4, 5\n",
    "\n",
    "    # Test shapes\n",
    "    M_y = torch.randn(k_y, d_out, d_out)\n",
    "    y = torch.randn(bsz, sl, d_out)\n",
    "    output = compute_ar_y(M_y, y)\n",
    "    assert output.shape == (bsz, sl, d_out)\n",
    "\n",
    "    # Test first timestep (should just be input)\n",
    "    assert torch.allclose(output[:, 0], y[:, 0])\n",
    "\n",
    "    # Test with identity matrices\n",
    "    M_y = torch.eye(d_out).unsqueeze(0).repeat(k_y, 1, 1)\n",
    "    y = torch.ones(bsz, sl, d_out)\n",
    "    output = compute_ar_y(M_y, y)\n",
    "    assert output.shape == (bsz, sl, d_out)\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run tests\n",
    "test_compute_ar_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "aopSEWFCx5Zw"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def nearest_power_of_2(x: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns the smallest power of 2 that is greater than or equal to x.\n",
    "    If x is already a power of 2, it returns x itself.\n",
    "    Otherwise, it returns the next higher power of 2.\n",
    "\n",
    "    Args:\n",
    "        x (int): The input integer.\n",
    "\n",
    "    Returns:\n",
    "        int: The smallest power of 2 that is greater than or equal to x.\n",
    "    \"\"\"\n",
    "    s = bin(x)\n",
    "    s = s.lstrip(\"-0b\")\n",
    "    length = len(s)\n",
    "    return 1 << (length - 1) if x == 1 << (length - 1) else 1 << length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "UwsOdhjux8Q4"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def conv(u: torch.Tensor, phi: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Implements the FFT convolution of the input sequences into the Hankel\n",
    "    spectral basis, as described in Section 3 of the paper.\n",
    "\n",
    "    This function computes U⁺_{t,k} and U⁻_{t,k}, which are the positive and\n",
    "    negative featurizations of the input sequence, respectively.\n",
    "\n",
    "    Args:\n",
    "        u (torch.Tensor): Input of shape [bsz, sl, d].\n",
    "        phi (torch.Tensor): Top K eigenvectors of shape [sl, K].\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Feature tensors U⁺ and U⁻ of shape [bsz, sl, K, d].\n",
    "    \"\"\"\n",
    "    bsz, sl, d = u.shape\n",
    "    _, K = phi.shape\n",
    "\n",
    "    # Round sequence length to the nearest power of 2 for efficient convolution\n",
    "    n = nearest_power_of_2(sl * 2 - 1)\n",
    "\n",
    "    # Add bsz and d dims to phi and u and expand to the return shape\n",
    "    phi = phi.view(1, -1, K, 1).expand(bsz, -1, K, d)\n",
    "    u = u.view(bsz, -1, 1, d).expand(bsz, -1, K, d)\n",
    "\n",
    "    # Compute U⁺\n",
    "    V = torch.fft.rfft(phi, n=n, dim=1)\n",
    "    U = torch.fft.rfft(u, n=n, dim=1)\n",
    "    U_plus = torch.fft.irfft(V * U, n=n, dim=1)[:, :sl]\n",
    "\n",
    "    # Generate alternating signs tensor, (-1)^i of length sl, match dims of u\n",
    "    alt = torch.ones(sl, device=u.device)\n",
    "    alt[1::2] = -1  # Replace every other element with -1, starting from index 1\n",
    "    alt = alt.view(1, sl, 1, 1).expand_as(u)\n",
    "\n",
    "    # Compute U⁻\n",
    "    u_alt = u * alt\n",
    "    U_alt = torch.fft.rfft(u_alt, n=n, dim=1)\n",
    "    U_minus = torch.fft.irfft(V * U_alt, n=n, dim=1)[:, :sl]\n",
    "\n",
    "    return U_plus, U_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "D1OPt-Dex-P1"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def compute_spectral(\n",
    "    inputs: torch.Tensor,\n",
    "    eigh: tuple[torch.Tensor, torch.Tensor],\n",
    "    M_phi_plus: torch.Tensor,\n",
    "    M_phi_minus: torch.Tensor,\n",
    "    M_y: torch.Tensor = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the spectral component of the STU or AR-STU model, as described\n",
    "    in Equations (4) and (6) of the paper.\n",
    "\n",
    "    This function projects the input tensor into the spectral basis via\n",
    "    convolution and applies the precomputed spectral filters.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): A tensor of shape [bsz, sl, d_in].\n",
    "        eigh (tuple[torch.Tensor, torch.Tensor]): Eigenvalues [K,] and eigenvectors [sl, K].\n",
    "        M_phi_plus (torch.Tensor): Positive spectral filter weights [d_out, K, d_in].\n",
    "        M_phi_minus (torch.Tensor): Negative spectral filter weights [d_out, K, d_in].\n",
    "        M_y (torch.Tensor, optional): Autoregressive weights for AR-STU [d_out, k_y, d_out].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The spectral component tensor of shape [bsz, sl, d_out].\n",
    "    \"\"\"\n",
    "    sigma, phi = eigh\n",
    "    _, K = phi.shape\n",
    "\n",
    "    # Compute U⁺ and U⁻\n",
    "    U_plus, U_minus = conv(inputs, phi)  # -> tuple of [bsz, sl, K, d_in]\n",
    "\n",
    "    # Shift U⁺ and U⁻ k_y time steps\n",
    "    if M_y is not None:\n",
    "        k_y = M_y.shape[1]\n",
    "        U_plus, U_minus = shift(U_plus, k_y), shift(U_minus, k_y)\n",
    "\n",
    "    # Perform spectral filter on U⁺ and U⁻ w/ sigma\n",
    "    sigma_root = (sigma**0.25).view(1, 1, K, 1)\n",
    "    U_plus_filtered, U_minus_filtered = U_plus * sigma_root, U_minus * sigma_root\n",
    "\n",
    "    # Sum M^{\\phi +}_k \\cdot U_plus_filtered across K filters\n",
    "    spectral_plus = torch.einsum(\"bsKd,dKo->bso\", U_plus_filtered, M_phi_plus)\n",
    "\n",
    "    # Sum M^{\\phi -}_k \\cdot U_minus_filtered across K filters\n",
    "    spectral_minus = torch.einsum(\"bsKd,dKo->bso\", U_minus_filtered, M_phi_minus)\n",
    "\n",
    "    return spectral_plus + spectral_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "H7w9I0sKyZVf"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SSSMConfigs:\n",
    "    d_in: int = 24\n",
    "    d_out: int = 18\n",
    "    n_layers: int = 6\n",
    "    n_embd: int = 512\n",
    "    sl: int = 300\n",
    "    scale: int = 4\n",
    "    bias: bool = False\n",
    "    dropout: float = 0.10\n",
    "    num_eigh: int = 24\n",
    "    k_u: int = 3  # Number of parametrizable, autoregressive matrices Mᵘ\n",
    "    k_y: int = 2  # Number of parametrizable, autoregressive matrices Mʸ\n",
    "    learnable_m_y: bool = True\n",
    "    alpha: float = 0.9  # 0.9 deemed \"uniformly optimal\" in the paper\n",
    "    use_hankel_L: bool = False\n",
    "    loss_fn: nn.Module = nn.MSELoss()\n",
    "    controls: dict = field(\n",
    "        default_factory=lambda: {\"task\": \"mujoco-v3\", \"controller\": \"Ant-v1\"}\n",
    "    )\n",
    "\n",
    "\n",
    "class STU(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple STU (Spectral Transform Unit) layer.\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object containing the following attributes:\n",
    "            d_in (int): Input dimension.\n",
    "            d_out (int): Output dimension.\n",
    "            sl (int): Input sequence length.\n",
    "            num_eigh (int): Number of spectral filters to use.\n",
    "            k_u (int): Autoregressive depth on the input sequence.\n",
    "            k_y (int): Autoregressive depth on the output sequence.\n",
    "            use_hankel_L (bool): Use the alternative Hankel matrix?\n",
    "            learnable_m_y (bool): Learn the M_y matrix?\n",
    "            dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs) -> None:\n",
    "        super(STU, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.d_in = configs.n_embd\n",
    "        self.d_out = configs.n_embd\n",
    "        self.l, self.k = configs.sl, configs.num_eigh\n",
    "        self.use_hankel_L = configs.use_hankel_L\n",
    "        self.eigh = get_top_eigh(self.l, self.k, self.use_hankel_L, self.device)\n",
    "        self.k_u = configs.k_u\n",
    "        self.k_y = configs.k_y\n",
    "        self.learnable_m_y = configs.learnable_m_y\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        # Parameterizable matrix Mᵘ, Mᵠ⁺, and Mᵠ⁻, per section 3\n",
    "        self.M_u = nn.Parameter(torch.empty(self.d_out, self.k_u, self.d_in))\n",
    "        self.M_phi_plus = nn.Parameter(torch.empty(self.d_out, self.k, self.d_in))\n",
    "        self.M_phi_minus = nn.Parameter(torch.empty(self.d_out, self.k, self.d_in))\n",
    "\n",
    "        # Parametrizable matrix Mʸ Introduced in section 5, equation 5\n",
    "        if self.learnable_m_y:\n",
    "            self.M_y = nn.Parameter(torch.zeros(self.d_out, self.k_y, self.d_out))\n",
    "        else:\n",
    "            self.register_buffer(\"M_y\", torch.zeros(self.d_out, self.k_y, self.d_out))\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the STU layer.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor of shape (bsz, sl, d_in)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (bsz, sl, d_out)\n",
    "        \"\"\"\n",
    "        y_t = compute_spectral(inputs, self.eigh, self.M_phi_plus, self.M_phi_minus, self.M_y)\n",
    "\n",
    "        if self.k_u > 0:\n",
    "          if self.k_y > 0:\n",
    "            y_t += compute_ar(self.M_y, self.M_u, inputs)\n",
    "          else:\n",
    "            y_t += compute_ar_u(self.M_u, inputs)\n",
    "\n",
    "        return self.dropout(y_t)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple multi-layer perceptron network using SwiGLU activation.\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object containing the following attributes:\n",
    "            scale (float): Scaling factor for hidden dimension.\n",
    "            n_embd (int): Embedding dimension.\n",
    "            bias (bool): Whether to use bias in linear layers.\n",
    "            dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs: SSSMConfigs) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.h_dim = configs.scale * configs.n_embd\n",
    "        self.swiglu = SwiGLU(dim=configs.n_embd, h_dim=self.h_dim, bias=configs.bias)\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the MLP.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.swiglu(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single block of the SSSM model, consisting of STU and MLP layers.\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object for STU and MLP layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs: SSSMConfigs) -> None:\n",
    "        super(Block, self).__init__()\n",
    "        # Good configuration\n",
    "        self.rn_1 = nn.RMSNorm(configs.n_embd)\n",
    "        self.stu = STU(configs)\n",
    "        self.rn_2 = nn.RMSNorm(configs.n_embd)\n",
    "        self.mlp = MLP(configs)\n",
    "\n",
    "        # Basic configuration\n",
    "        # self.stu = STU(configs) #this also has a size problem [d_in,d_out] instead of [d_emb,  d_emb]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        # Good configuration\n",
    "        x = x + self.stu(self.rn_1(x))\n",
    "        x = x + self.mlp(self.rn_2(x))\n",
    "\n",
    "        # Basic configuration (no skips, no non-linearities)\n",
    "        # x = self.stu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    General model architecture based on stacked STU blocks and MLP layers.\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object containing model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs: SSSMConfigs) -> None:\n",
    "        super(SSSM, self).__init__()\n",
    "        self.configs = configs\n",
    "        self.n_layers = configs.n_layers\n",
    "        self.n_embd = configs.n_embd\n",
    "        self.d_in = configs.d_in\n",
    "        self.d_out = configs.d_out\n",
    "        self.sl = configs.sl\n",
    "        self.learnable_m_y = configs.learnable_m_y\n",
    "        self.alpha = configs.alpha\n",
    "\n",
    "        self.bias = configs.bias\n",
    "        self.dropout = configs.dropout\n",
    "        self.loss_fn = configs.loss_fn\n",
    "        self.controls = configs.controls\n",
    "\n",
    "        self.emb = nn.Linear(self.d_in, self.n_embd, bias=self.bias)\n",
    "        self.stu = nn.ModuleDict(\n",
    "            dict(\n",
    "                dropout=nn.Dropout(self.dropout),\n",
    "                hidden=nn.ModuleList([Block(configs) for _ in range(self.n_layers)]),\n",
    "            )\n",
    "        )\n",
    "        self.task_head = nn.Linear(self.n_embd, self.d_out, bias=self.bias)\n",
    "\n",
    "        # Initialize all weights\n",
    "        self.m_x = self.d_out**-0.5\n",
    "        self.std = self.n_embd**-0.5\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Report the number of parameters\n",
    "        print(\"STU Model Parameter Count: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "\n",
    "    def forward(self, inputs, targets = None):\n",
    "        \"\"\"\n",
    "        Forward pass of the SSSM model.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor of shape (bsz, sl, d_in)\n",
    "            targets (torch.Tensor): Target tensor for loss computation\n",
    "\n",
    "        Returns:\n",
    "            Type (ignore due to high variability):\n",
    "            - Predictions tensor\n",
    "            - Tuple containing loss and metrics (if applicable)\n",
    "        \"\"\"\n",
    "        # _, sl, n_embd = inputs.size()\n",
    "\n",
    "        x = self.emb(inputs)\n",
    "        x = self.stu.dropout(x)\n",
    "\n",
    "        for block in self.stu.hidden:\n",
    "            x = block(x)\n",
    "\n",
    "        preds = self.task_head(x)\n",
    "\n",
    "        loss = self.loss_fn(preds, targets) if targets is not None else None\n",
    "        return ((preds, loss) if loss is not None else preds)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the model.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): The module to initialize.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            self.std *= (2 * self.n_layers) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.std)\n",
    "        elif isinstance(module, STU):\n",
    "            torch.nn.init.uniform_(module.M_u, -self.m_x, self.m_x)\n",
    "            torch.nn.init.zeros_(module.M_phi_plus)\n",
    "            torch.nn.init.zeros_(module.M_phi_minus)\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "\n",
    "        Args:\n",
    "            non_embedding (bool, optional):\n",
    "            Whether to exclude the positional embeddings (if applicable).\n",
    "            Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        return num_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8RJEm4Z0hQ_",
    "outputId": "cd7bcf1b-1cb4-4e81-ac4e-7e36bdd095ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STU Model Parameter Count: 0.01M\n"
     ]
    }
   ],
   "source": [
    "bsz=1 # @param\n",
    "n_layers=2 # @param\n",
    "n_embd=8 # @param\n",
    "d_in=24 # @param\n",
    "d_out=18 # @param\n",
    "sl=512 # @param\n",
    "scale=4 # @param\n",
    "bias=False # @param\n",
    "dropout=0.0 # @param\n",
    "num_eigh=16 # @param\n",
    "k_u=3 # @param\n",
    "k_y=0 # @param\n",
    "learnable_m_y=True # @param\n",
    "alpha=0.9 # @param\n",
    "use_hankel_L=False # @param\n",
    "loss_fn=nn.MSELoss() # @param\n",
    "lr=1e-1 # @param\n",
    "delta=0.01 # @param\n",
    "\n",
    "configs = SSSMConfigs(\n",
    "  n_layers=n_layers,\n",
    "  n_embd=n_embd,\n",
    "  d_in=d_in,\n",
    "  d_out=d_out,\n",
    "  sl=sl,\n",
    "  scale=scale,\n",
    "  bias=bias,\n",
    "  dropout=dropout,\n",
    "  num_eigh=num_eigh,\n",
    "  k_u=k_u,\n",
    "  k_y=k_y,\n",
    "  learnable_m_y=learnable_m_y,\n",
    "  alpha=alpha,\n",
    "  use_hankel_L=use_hankel_L,\n",
    "  loss_fn=loss_fn,\n",
    ")\n",
    "\n",
    "model = SSSM(configs).to(device)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\n",
      "Creating dataloader on cuda for task: mujoco-v1\u001b[0m\n",
      "Using sequence length: 512\n",
      "\n",
      "Apply Gaussian noise to data?: Disabled\n",
      "\u001b[94m\n",
      "Calculating data statistics...\u001b[0m\n",
      "\u001b[94mNormalizing data...\u001b[0m\n",
      "\u001b[94mValidating data normalization...\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for coordinates: [0.00018377 0.0004239 ]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for coordinates: [0.99988493 1.00014359]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for coordinates: [-0.00018377 -0.0004239 ]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for coordinates: [1.00011158 0.99980374]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for angles: [-0.00020589 -0.00096165 -0.00018324  0.00073418 -0.00010059 -0.00011929\n",
      " -0.00011729]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for angles: [1.00041433 1.00185566 1.00013121 1.00021316 1.00018604 1.00035338\n",
      " 1.00017155]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for angles: [ 0.00020589  0.00096165  0.00018324 -0.00073418  0.00010059  0.00011929\n",
      "  0.00011729]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for angles: [0.99954492 0.9977162  0.9997683  0.9997608  0.99939967 0.99954043\n",
      " 0.9998055 ]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for coordinate_velocities: [ 0.00023379 -0.00012828]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for coordinate_velocities: [1.00050389 1.00003277]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for coordinate_velocities: [-0.00023379  0.00012828]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for coordinate_velocities: [0.99948129 0.99995449]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for angular_velocities: [ 3.28207562e-05  2.49162060e-05 -1.21271749e-05 -3.17272072e-04\n",
      "  1.78593172e-05 -5.56011537e-06  2.38656492e-04]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for angular_velocities: [1.00014932 1.00054625 1.00016789 1.00032316 0.99962519 1.00011136\n",
      " 0.99996271]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for angular_velocities: [-3.28207562e-05 -2.49162060e-05  1.21271749e-05  3.17272072e-04\n",
      " -1.78593172e-05  5.56011537e-06 -2.38656492e-04]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for angular_velocities: [0.99984485 0.99944538 0.99982847 0.99967394 1.00036605 0.99988489\n",
      " 1.00003486]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for torque: [-4.62952337e-17 -4.98681692e-17 -5.88959294e-18 -6.86438376e-16\n",
      " -8.99262482e-17  5.87139334e-17]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for torque: [0.99997337 0.99997864 0.99998755 0.99977441 0.99997176 0.99998938]\u001b[0m\n",
      "\u001b[1m\u001b[92mData normalization validated successfully.\u001b[0m\n",
      "\u001b[92mDataloader created successfully.\u001b[0m\n",
      "\u001b[94m\n",
      "Creating dataloader on cuda for task: mujoco-v1\u001b[0m\n",
      "Using sequence length: 512\n",
      "\n",
      "Apply Gaussian noise to data?: Disabled\n",
      "\u001b[94m\n",
      "Calculating data statistics...\u001b[0m\n",
      "\u001b[94mNormalizing data...\u001b[0m\n",
      "\u001b[94mValidating data normalization...\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for coordinates: [-0.00073541 -0.00169415]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for coordinates: [1.00045294 0.99929953]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for coordinates: [0.00073541 0.00169415]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for coordinates: [0.99954287 1.00064472]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for angles: [ 0.00082333  0.00383343  0.00073323 -0.00293492  0.00040419  0.00047672\n",
      "  0.00046875]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for angles: [0.99824278 0.99149925 0.99922457 0.99908021 0.99821533 0.99831713\n",
      " 0.99925645]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for angles: [-0.00082333 -0.00383343 -0.00073323  0.00293492 -0.00040419 -0.00047672\n",
      " -0.00046875]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for angles: [1.00171304 1.0079956  1.00067394 1.00088491 1.00136749 1.00157388\n",
      " 1.00071988]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for coordinate_velocities: [-0.0009364   0.00051173]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for coordinate_velocities: [0.99793967 0.99983744]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for coordinate_velocities: [ 0.0009364  -0.00051173]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for coordinate_velocities: [1.00204072 1.00014959]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for angular_velocities: [-1.31102936e-04 -9.94288561e-05  4.84091778e-05  1.26707851e-03\n",
      " -7.12301366e-05  2.21757815e-05 -9.53706499e-04]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for angular_velocities: [0.99938935 0.99780371 0.99932105 0.99870094 1.00146769 0.99954576\n",
      " 1.00014297]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for angular_velocities: [ 1.31102936e-04  9.94288561e-05 -4.84091778e-05 -1.26707851e-03\n",
      "  7.12301366e-05 -2.21757815e-05  9.53706499e-04]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for angular_velocities: [1.00060446 1.00218343 1.00067489 1.00129309 0.99852153 1.00045031\n",
      " 0.99985372]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for torque: [ 4.51998167e-17  1.74249741e-17 -2.87451428e-17 -6.49171249e-16\n",
      " -6.95988233e-17  3.67905485e-17]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for torque: [0.99997357 0.99997865 0.99998755 0.99977556 0.99997179 0.99998939]\u001b[0m\n",
      "\u001b[1m\u001b[92mData normalization validated successfully.\u001b[0m\n",
      "\u001b[92mDataloader created successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "controller = 'Walker2D-v1' #@param\n",
    "train_data = {\n",
    "    \"inputs\": f\"data/mujoco-v1/{controller}/train_inputs.npy\",\n",
    "    \"targets\": f\"data/mujoco-v1/{controller}/train_targets.npy\",\n",
    "}\n",
    "val_data = {\n",
    "    \"inputs\": f\"data/mujoco-v1/{controller}/val_inputs.npy\",\n",
    "    \"targets\": f\"data/mujoco-v1/{controller}/val_targets.npy\",\n",
    "}\n",
    "\n",
    "from utils.dataloader import get_dataloader \n",
    "\n",
    "train_loader = get_dataloader(\n",
    "    model  = 'spectral_ssm',\n",
    "    data = train_data,\n",
    "    task = 'mujoco-v1',\n",
    "    controller = controller,\n",
    "    bsz  = bsz,\n",
    "    preprocess =  True, #normalize\n",
    "    shuffle= True,\n",
    "    sl = sl,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "val_loader = get_dataloader(\n",
    "    model  = 'spectral_ssm',\n",
    "    data = val_data,\n",
    "    task = 'mujoco-v1',\n",
    "    controller =  controller,\n",
    "    bsz = bsz,\n",
    "    preprocess =  True, #normalize\n",
    "    shuffle= True,\n",
    "    sl = sl,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using batch size: 1\n",
      "Number of epochs: 1\n",
      "Steps per epoch: 2811\n",
      "=> Number of training steps: 2811\n"
     ]
    }
   ],
   "source": [
    "training_stu = True #@param\n",
    "num_epochs: int = 1 #@param\n",
    "steps_per_epoch = len(train_loader) #@param\n",
    "num_steps: int = steps_per_epoch * num_epochs #@param\n",
    "dilation: int = 1 #@param\n",
    "warmup_steps: int = num_steps // 8 #@param\n",
    "eval_period: int = num_steps // 16 #@param\n",
    "\n",
    "print(f\"\\nUsing batch size: {bsz}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"=> Number of training steps: {num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from utils.colors import Colors, colored_print\n",
    "from losses.loss_ant import AntLoss\n",
    "from losses.loss_cheetah import HalfCheetahLoss\n",
    "from losses.loss_walker import Walker2DLoss\n",
    "from losses.loss_cartpole import CartpoleLoss\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "checkpoint_dir: str = \"checkpoints\"\n",
    "if controller == \"Ant-v1\":\n",
    "    loss_fn = AntLoss()\n",
    "elif controller == \"HalfCheetah-v1\":\n",
    "    loss_fn = HalfCheetahLoss()\n",
    "elif controller == \"Walker2D-v1\":\n",
    "    loss_fn = Walker2DLoss()\n",
    "elif controller == \"CartPole-v1\":\n",
    "    loss_fn = CartpoleLoss()\n",
    "else:\n",
    "    loss_fn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_counter = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_step = 0\n",
    "best_checkpoint = None\n",
    "patience: int = 10 #number of  non-improving eval periods before early stoppig\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_time_steps = []\n",
    "grad_norms = []\n",
    "\n",
    "model  = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 0.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8502.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8501.\u001b[0m\n",
      "\u001b[95m\n",
      "Step     0\u001b[0m\n",
      "\u001b[94mLoss: 0.765131 | Gradient Norm: 0.1048\u001b[0m\n",
      "\u001b[95m\n",
      "Step    10\u001b[0m\n",
      "\u001b[94mLoss: 0.740599 | Gradient Norm: 0.1574\u001b[0m\n",
      "\u001b[95m\n",
      "Step    20\u001b[0m\n",
      "\u001b[94mLoss: 0.887945 | Gradient Norm: 0.3049\u001b[0m\n",
      "\u001b[95m\n",
      "Step    30\u001b[0m\n",
      "\u001b[94mLoss: 0.934229 | Gradient Norm: 0.2788\u001b[0m\n",
      "\u001b[95m\n",
      "Step    40\u001b[0m\n",
      "\u001b[94mLoss: 0.754667 | Gradient Norm: 0.1731\u001b[0m\n",
      "\u001b[95m\n",
      "Step    50\u001b[0m\n",
      "\u001b[94mLoss: 0.808573 | Gradient Norm: 0.1272\u001b[0m\n",
      "\u001b[95m\n",
      "Step    60\u001b[0m\n",
      "\u001b[94mLoss: 0.998579 | Gradient Norm: 0.3215\u001b[0m\n",
      "\u001b[95m\n",
      "Step    70\u001b[0m\n",
      "\u001b[94mLoss: 0.949057 | Gradient Norm: 0.1529\u001b[0m\n",
      "\u001b[95m\n",
      "Step    80\u001b[0m\n",
      "\u001b[94mLoss: 0.791073 | Gradient Norm: 0.2070\u001b[0m\n",
      "\u001b[95m\n",
      "Step    90\u001b[0m\n",
      "\u001b[94mLoss: 0.742288 | Gradient Norm: 0.1725\u001b[0m\n",
      "\u001b[95m\n",
      "Step   100\u001b[0m\n",
      "\u001b[94mLoss: 0.791478 | Gradient Norm: 0.1243\u001b[0m\n",
      "\u001b[95m\n",
      "Step   110\u001b[0m\n",
      "\u001b[94mLoss: 1.115946 | Gradient Norm: 0.2458\u001b[0m\n",
      "\u001b[95m\n",
      "Step   120\u001b[0m\n",
      "\u001b[94mLoss: 0.971060 | Gradient Norm: 0.1712\u001b[0m\n",
      "\u001b[95m\n",
      "Step   130\u001b[0m\n",
      "\u001b[94mLoss: 0.737454 | Gradient Norm: 0.1752\u001b[0m\n",
      "\u001b[95m\n",
      "Step   140\u001b[0m\n",
      "\u001b[94mLoss: 0.897802 | Gradient Norm: 0.1991\u001b[0m\n",
      "\u001b[95m\n",
      "Step   150\u001b[0m\n",
      "\u001b[94mLoss: 0.708089 | Gradient Norm: 0.2631\u001b[0m\n",
      "\u001b[95m\n",
      "Step   160\u001b[0m\n",
      "\u001b[94mLoss: 0.816766 | Gradient Norm: 0.1230\u001b[0m\n",
      "\u001b[95m\n",
      "Step   170\u001b[0m\n",
      "\u001b[94mLoss: 0.797774 | Gradient Norm: 0.2567\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 175.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8507.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8501.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   180\u001b[0m\n",
      "\u001b[94mLoss: 0.915517 | Gradient Norm: 0.1598\u001b[0m\n",
      "\u001b[95m\n",
      "Step   190\u001b[0m\n",
      "\u001b[94mLoss: 0.739996 | Gradient Norm: 0.2836\u001b[0m\n",
      "\u001b[95m\n",
      "Step   200\u001b[0m\n",
      "\u001b[94mLoss: 0.782978 | Gradient Norm: 0.1167\u001b[0m\n",
      "\u001b[95m\n",
      "Step   210\u001b[0m\n",
      "\u001b[94mLoss: 0.854887 | Gradient Norm: 0.2661\u001b[0m\n",
      "\u001b[95m\n",
      "Step   220\u001b[0m\n",
      "\u001b[94mLoss: 0.849160 | Gradient Norm: 0.1892\u001b[0m\n",
      "\u001b[95m\n",
      "Step   230\u001b[0m\n",
      "\u001b[94mLoss: 0.727241 | Gradient Norm: 0.1773\u001b[0m\n",
      "\u001b[95m\n",
      "Step   240\u001b[0m\n",
      "\u001b[94mLoss: 0.949745 | Gradient Norm: 0.2427\u001b[0m\n",
      "\u001b[95m\n",
      "Step   250\u001b[0m\n",
      "\u001b[94mLoss: 0.975289 | Gradient Norm: 0.2908\u001b[0m\n",
      "\u001b[95m\n",
      "Step   260\u001b[0m\n",
      "\u001b[94mLoss: 1.048794 | Gradient Norm: 0.1648\u001b[0m\n",
      "\u001b[95m\n",
      "Step   270\u001b[0m\n",
      "\u001b[94mLoss: 0.779358 | Gradient Norm: 0.2206\u001b[0m\n",
      "\u001b[95m\n",
      "Step   280\u001b[0m\n",
      "\u001b[94mLoss: 0.767511 | Gradient Norm: 0.1288\u001b[0m\n",
      "\u001b[95m\n",
      "Step   290\u001b[0m\n",
      "\u001b[94mLoss: 0.848589 | Gradient Norm: 0.1907\u001b[0m\n",
      "\u001b[95m\n",
      "Step   300\u001b[0m\n",
      "\u001b[94mLoss: 0.767883 | Gradient Norm: 0.1887\u001b[0m\n",
      "\u001b[95m\n",
      "Step   310\u001b[0m\n",
      "\u001b[94mLoss: 0.846393 | Gradient Norm: 0.1849\u001b[0m\n",
      "\u001b[95m\n",
      "Step   320\u001b[0m\n",
      "\u001b[94mLoss: 0.708990 | Gradient Norm: 0.3392\u001b[0m\n",
      "\u001b[95m\n",
      "Step   330\u001b[0m\n",
      "\u001b[94mLoss: 0.880755 | Gradient Norm: 0.1622\u001b[0m\n",
      "\u001b[95m\n",
      "Step   340\u001b[0m\n",
      "\u001b[94mLoss: 0.838698 | Gradient Norm: 0.1613\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 350.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8505.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8501.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   350\u001b[0m\n",
      "\u001b[94mLoss: 0.864869 | Gradient Norm: 0.1551\u001b[0m\n",
      "\u001b[95m\n",
      "Step   360\u001b[0m\n",
      "\u001b[94mLoss: 0.931529 | Gradient Norm: 0.2101\u001b[0m\n",
      "\u001b[95m\n",
      "Step   370\u001b[0m\n",
      "\u001b[94mLoss: 0.754449 | Gradient Norm: 0.1887\u001b[0m\n",
      "\u001b[95m\n",
      "Step   380\u001b[0m\n",
      "\u001b[94mLoss: 0.879484 | Gradient Norm: 0.0922\u001b[0m\n",
      "\u001b[95m\n",
      "Step   390\u001b[0m\n",
      "\u001b[94mLoss: 0.794521 | Gradient Norm: 0.3486\u001b[0m\n",
      "\u001b[95m\n",
      "Step   400\u001b[0m\n",
      "\u001b[94mLoss: 1.115235 | Gradient Norm: 0.2809\u001b[0m\n",
      "\u001b[95m\n",
      "Step   410\u001b[0m\n",
      "\u001b[94mLoss: 0.853975 | Gradient Norm: 0.2029\u001b[0m\n",
      "\u001b[95m\n",
      "Step   420\u001b[0m\n",
      "\u001b[94mLoss: 0.820576 | Gradient Norm: 0.1503\u001b[0m\n",
      "\u001b[95m\n",
      "Step   430\u001b[0m\n",
      "\u001b[94mLoss: 0.795982 | Gradient Norm: 0.1731\u001b[0m\n",
      "\u001b[95m\n",
      "Step   440\u001b[0m\n",
      "\u001b[94mLoss: 0.916814 | Gradient Norm: 0.1996\u001b[0m\n",
      "\u001b[95m\n",
      "Step   450\u001b[0m\n",
      "\u001b[94mLoss: 0.779418 | Gradient Norm: 0.2172\u001b[0m\n",
      "\u001b[95m\n",
      "Step   460\u001b[0m\n",
      "\u001b[94mLoss: 1.096080 | Gradient Norm: 0.2702\u001b[0m\n",
      "\u001b[95m\n",
      "Step   470\u001b[0m\n",
      "\u001b[94mLoss: 0.912810 | Gradient Norm: 0.1464\u001b[0m\n",
      "\u001b[95m\n",
      "Step   480\u001b[0m\n",
      "\u001b[94mLoss: 0.920880 | Gradient Norm: 0.3415\u001b[0m\n",
      "\u001b[95m\n",
      "Step   490\u001b[0m\n",
      "\u001b[94mLoss: 0.750817 | Gradient Norm: 0.2032\u001b[0m\n",
      "\u001b[95m\n",
      "Step   500\u001b[0m\n",
      "\u001b[94mLoss: 0.948923 | Gradient Norm: 0.3463\u001b[0m\n",
      "\u001b[95m\n",
      "Step   510\u001b[0m\n",
      "\u001b[94mLoss: 1.534805 | Gradient Norm: 0.3424\u001b[0m\n",
      "\u001b[95m\n",
      "Step   520\u001b[0m\n",
      "\u001b[94mLoss: 0.940357 | Gradient Norm: 0.2310\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 525.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8519.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8501.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   530\u001b[0m\n",
      "\u001b[94mLoss: 1.019074 | Gradient Norm: 0.1428\u001b[0m\n",
      "\u001b[95m\n",
      "Step   540\u001b[0m\n",
      "\u001b[94mLoss: 0.928574 | Gradient Norm: 0.2070\u001b[0m\n",
      "\u001b[95m\n",
      "Step   550\u001b[0m\n",
      "\u001b[94mLoss: 0.805397 | Gradient Norm: 0.1358\u001b[0m\n",
      "\u001b[95m\n",
      "Step   560\u001b[0m\n",
      "\u001b[94mLoss: 0.747311 | Gradient Norm: 0.2014\u001b[0m\n",
      "\u001b[95m\n",
      "Step   570\u001b[0m\n",
      "\u001b[94mLoss: 0.796023 | Gradient Norm: 0.1857\u001b[0m\n",
      "\u001b[95m\n",
      "Step   580\u001b[0m\n",
      "\u001b[94mLoss: 0.839567 | Gradient Norm: 0.3246\u001b[0m\n",
      "\u001b[95m\n",
      "Step   590\u001b[0m\n",
      "\u001b[94mLoss: 0.875888 | Gradient Norm: 0.1613\u001b[0m\n",
      "\u001b[95m\n",
      "Step   600\u001b[0m\n",
      "\u001b[94mLoss: 0.753182 | Gradient Norm: 0.2448\u001b[0m\n",
      "\u001b[95m\n",
      "Step   610\u001b[0m\n",
      "\u001b[94mLoss: 0.808898 | Gradient Norm: 0.0999\u001b[0m\n",
      "\u001b[95m\n",
      "Step   620\u001b[0m\n",
      "\u001b[94mLoss: 0.785085 | Gradient Norm: 0.2413\u001b[0m\n",
      "\u001b[95m\n",
      "Step   630\u001b[0m\n",
      "\u001b[94mLoss: 0.774569 | Gradient Norm: 0.1620\u001b[0m\n",
      "\u001b[95m\n",
      "Step   640\u001b[0m\n",
      "\u001b[94mLoss: 0.773915 | Gradient Norm: 0.1744\u001b[0m\n",
      "\u001b[95m\n",
      "Step   650\u001b[0m\n",
      "\u001b[94mLoss: 0.764506 | Gradient Norm: 0.2061\u001b[0m\n",
      "\u001b[95m\n",
      "Step   660\u001b[0m\n",
      "\u001b[94mLoss: 0.802648 | Gradient Norm: 0.1891\u001b[0m\n",
      "\u001b[95m\n",
      "Step   670\u001b[0m\n",
      "\u001b[94mLoss: 0.720587 | Gradient Norm: 0.3507\u001b[0m\n",
      "\u001b[95m\n",
      "Step   680\u001b[0m\n",
      "\u001b[94mLoss: 0.881445 | Gradient Norm: 0.1807\u001b[0m\n",
      "\u001b[95m\n",
      "Step   690\u001b[0m\n",
      "\u001b[94mLoss: 1.095066 | Gradient Norm: 0.1928\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 700.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8519.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8501.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   700\u001b[0m\n",
      "\u001b[94mLoss: 0.827739 | Gradient Norm: 0.1754\u001b[0m\n",
      "\u001b[95m\n",
      "Step   710\u001b[0m\n",
      "\u001b[94mLoss: 0.981120 | Gradient Norm: 0.2314\u001b[0m\n",
      "\u001b[95m\n",
      "Step   720\u001b[0m\n",
      "\u001b[94mLoss: 0.848535 | Gradient Norm: 0.2428\u001b[0m\n",
      "\u001b[95m\n",
      "Step   730\u001b[0m\n",
      "\u001b[94mLoss: 0.724185 | Gradient Norm: 0.1455\u001b[0m\n",
      "\u001b[95m\n",
      "Step   740\u001b[0m\n",
      "\u001b[94mLoss: 0.704233 | Gradient Norm: 0.2433\u001b[0m\n",
      "\u001b[95m\n",
      "Step   750\u001b[0m\n",
      "\u001b[94mLoss: 0.879963 | Gradient Norm: 0.1265\u001b[0m\n",
      "\u001b[95m\n",
      "Step   760\u001b[0m\n",
      "\u001b[94mLoss: 0.668180 | Gradient Norm: 0.1673\u001b[0m\n",
      "\u001b[95m\n",
      "Step   770\u001b[0m\n",
      "\u001b[94mLoss: 0.812863 | Gradient Norm: 0.2937\u001b[0m\n",
      "\u001b[95m\n",
      "Step   780\u001b[0m\n",
      "\u001b[94mLoss: 0.752751 | Gradient Norm: 0.2530\u001b[0m\n",
      "\u001b[95m\n",
      "Step   790\u001b[0m\n",
      "\u001b[94mLoss: 0.858233 | Gradient Norm: 0.1174\u001b[0m\n",
      "\u001b[95m\n",
      "Step   800\u001b[0m\n",
      "\u001b[94mLoss: 0.852124 | Gradient Norm: 0.1794\u001b[0m\n",
      "\u001b[95m\n",
      "Step   810\u001b[0m\n",
      "\u001b[94mLoss: 1.025255 | Gradient Norm: 0.2654\u001b[0m\n",
      "\u001b[95m\n",
      "Step   820\u001b[0m\n",
      "\u001b[94mLoss: 0.826472 | Gradient Norm: 0.1412\u001b[0m\n",
      "\u001b[95m\n",
      "Step   830\u001b[0m\n",
      "\u001b[94mLoss: 0.726639 | Gradient Norm: 0.2576\u001b[0m\n",
      "\u001b[95m\n",
      "Step   840\u001b[0m\n",
      "\u001b[94mLoss: 0.829865 | Gradient Norm: 0.1437\u001b[0m\n",
      "\u001b[95m\n",
      "Step   850\u001b[0m\n",
      "\u001b[94mLoss: 0.836872 | Gradient Norm: 0.2268\u001b[0m\n",
      "\u001b[95m\n",
      "Step   860\u001b[0m\n",
      "\u001b[94mLoss: 0.896833 | Gradient Norm: 0.1782\u001b[0m\n",
      "\u001b[95m\n",
      "Step   870\u001b[0m\n",
      "\u001b[94mLoss: 0.819165 | Gradient Norm: 0.2223\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 875.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8507.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8501.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   880\u001b[0m\n",
      "\u001b[94mLoss: 0.783884 | Gradient Norm: 0.1724\u001b[0m\n",
      "\u001b[95m\n",
      "Step   890\u001b[0m\n",
      "\u001b[94mLoss: 0.634690 | Gradient Norm: 0.2921\u001b[0m\n",
      "\u001b[95m\n",
      "Step   900\u001b[0m\n",
      "\u001b[94mLoss: 0.905363 | Gradient Norm: 0.2442\u001b[0m\n",
      "\u001b[95m\n",
      "Step   910\u001b[0m\n",
      "\u001b[94mLoss: 1.025754 | Gradient Norm: 0.1896\u001b[0m\n",
      "\u001b[95m\n",
      "Step   920\u001b[0m\n",
      "\u001b[94mLoss: 0.915139 | Gradient Norm: 0.2200\u001b[0m\n",
      "\u001b[95m\n",
      "Step   930\u001b[0m\n",
      "\u001b[94mLoss: 0.988646 | Gradient Norm: 0.1933\u001b[0m\n",
      "\u001b[95m\n",
      "Step   940\u001b[0m\n",
      "\u001b[94mLoss: 0.746339 | Gradient Norm: 0.1408\u001b[0m\n",
      "\u001b[95m\n",
      "Step   950\u001b[0m\n",
      "\u001b[94mLoss: 0.760216 | Gradient Norm: 0.1888\u001b[0m\n",
      "\u001b[95m\n",
      "Step   960\u001b[0m\n",
      "\u001b[94mLoss: 0.756623 | Gradient Norm: 0.1286\u001b[0m\n",
      "\u001b[95m\n",
      "Step   970\u001b[0m\n",
      "\u001b[94mLoss: 1.086055 | Gradient Norm: 0.2800\u001b[0m\n",
      "\u001b[95m\n",
      "Step   980\u001b[0m\n",
      "\u001b[94mLoss: 1.060663 | Gradient Norm: 0.3625\u001b[0m\n",
      "\u001b[95m\n",
      "Step   990\u001b[0m\n",
      "\u001b[94mLoss: 0.943470 | Gradient Norm: 0.2194\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1000\u001b[0m\n",
      "\u001b[94mLoss: 0.818199 | Gradient Norm: 0.1471\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1010\u001b[0m\n",
      "\u001b[94mLoss: 0.979636 | Gradient Norm: 0.2681\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1020\u001b[0m\n",
      "\u001b[94mLoss: 0.625319 | Gradient Norm: 0.2339\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1030\u001b[0m\n",
      "\u001b[94mLoss: 1.055940 | Gradient Norm: 0.2507\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1040\u001b[0m\n",
      "\u001b[94mLoss: 0.814037 | Gradient Norm: 0.2110\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1050.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8496.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8496. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-1050-2024-11-18-16-34-38.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1050\u001b[0m\n",
      "\u001b[94mLoss: 0.787567 | Gradient Norm: 0.1224\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1060\u001b[0m\n",
      "\u001b[94mLoss: 0.833782 | Gradient Norm: 0.2251\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1070\u001b[0m\n",
      "\u001b[94mLoss: 0.762755 | Gradient Norm: 0.1779\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1080\u001b[0m\n",
      "\u001b[94mLoss: 0.936548 | Gradient Norm: 0.1896\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1090\u001b[0m\n",
      "\u001b[94mLoss: 0.858079 | Gradient Norm: 0.1889\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1100\u001b[0m\n",
      "\u001b[94mLoss: 0.826643 | Gradient Norm: 0.1359\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1110\u001b[0m\n",
      "\u001b[94mLoss: 0.796249 | Gradient Norm: 0.1698\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1120\u001b[0m\n",
      "\u001b[94mLoss: 0.928550 | Gradient Norm: 0.1820\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1130\u001b[0m\n",
      "\u001b[94mLoss: 0.710058 | Gradient Norm: 0.1422\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1140\u001b[0m\n",
      "\u001b[94mLoss: 0.984525 | Gradient Norm: 0.2509\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1150\u001b[0m\n",
      "\u001b[94mLoss: 0.819051 | Gradient Norm: 0.1442\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1160\u001b[0m\n",
      "\u001b[94mLoss: 0.815586 | Gradient Norm: 0.1813\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1170\u001b[0m\n",
      "\u001b[94mLoss: 0.745218 | Gradient Norm: 0.1217\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1180\u001b[0m\n",
      "\u001b[94mLoss: 0.694132 | Gradient Norm: 0.1809\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1190\u001b[0m\n",
      "\u001b[94mLoss: 0.844206 | Gradient Norm: 0.1825\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1200\u001b[0m\n",
      "\u001b[94mLoss: 0.892597 | Gradient Norm: 0.1427\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1210\u001b[0m\n",
      "\u001b[94mLoss: 0.825058 | Gradient Norm: 0.2985\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1220\u001b[0m\n",
      "\u001b[94mLoss: 0.794908 | Gradient Norm: 0.1408\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1225.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8506.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8496.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1230\u001b[0m\n",
      "\u001b[94mLoss: 0.910433 | Gradient Norm: 0.2049\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1240\u001b[0m\n",
      "\u001b[94mLoss: 0.917792 | Gradient Norm: 0.2337\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1250\u001b[0m\n",
      "\u001b[94mLoss: 0.883672 | Gradient Norm: 0.1996\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1260\u001b[0m\n",
      "\u001b[94mLoss: 0.722332 | Gradient Norm: 0.2149\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1270\u001b[0m\n",
      "\u001b[94mLoss: 0.760134 | Gradient Norm: 0.1226\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1280\u001b[0m\n",
      "\u001b[94mLoss: 0.737287 | Gradient Norm: 0.1076\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1290\u001b[0m\n",
      "\u001b[94mLoss: 0.607308 | Gradient Norm: 0.2692\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1300\u001b[0m\n",
      "\u001b[94mLoss: 0.827662 | Gradient Norm: 0.2170\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1310\u001b[0m\n",
      "\u001b[94mLoss: 0.742717 | Gradient Norm: 0.1720\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1320\u001b[0m\n",
      "\u001b[94mLoss: 1.231347 | Gradient Norm: 0.3714\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1330\u001b[0m\n",
      "\u001b[94mLoss: 0.858104 | Gradient Norm: 0.1860\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1340\u001b[0m\n",
      "\u001b[94mLoss: 1.015343 | Gradient Norm: 0.1729\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1350\u001b[0m\n",
      "\u001b[94mLoss: 0.787397 | Gradient Norm: 0.1353\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1360\u001b[0m\n",
      "\u001b[94mLoss: 0.879303 | Gradient Norm: 0.2279\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1370\u001b[0m\n",
      "\u001b[94mLoss: 1.041982 | Gradient Norm: 0.3060\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1380\u001b[0m\n",
      "\u001b[94mLoss: 0.770829 | Gradient Norm: 0.1261\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1390\u001b[0m\n",
      "\u001b[94mLoss: 0.793061 | Gradient Norm: 0.1411\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1400.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8522.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8496.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1400\u001b[0m\n",
      "\u001b[94mLoss: 0.845131 | Gradient Norm: 0.1717\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1410\u001b[0m\n",
      "\u001b[94mLoss: 0.848811 | Gradient Norm: 0.1771\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1420\u001b[0m\n",
      "\u001b[94mLoss: 0.790451 | Gradient Norm: 0.1371\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1430\u001b[0m\n",
      "\u001b[94mLoss: 0.919832 | Gradient Norm: 0.1941\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1440\u001b[0m\n",
      "\u001b[94mLoss: 0.788599 | Gradient Norm: 0.1265\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1450\u001b[0m\n",
      "\u001b[94mLoss: 0.653287 | Gradient Norm: 0.1597\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1460\u001b[0m\n",
      "\u001b[94mLoss: 0.796445 | Gradient Norm: 0.2631\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1470\u001b[0m\n",
      "\u001b[94mLoss: 0.771079 | Gradient Norm: 0.2261\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1480\u001b[0m\n",
      "\u001b[94mLoss: 0.763857 | Gradient Norm: 0.1228\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1490\u001b[0m\n",
      "\u001b[94mLoss: 1.220253 | Gradient Norm: 0.2693\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1500\u001b[0m\n",
      "\u001b[94mLoss: 0.793526 | Gradient Norm: 0.1942\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1510\u001b[0m\n",
      "\u001b[94mLoss: 0.796157 | Gradient Norm: 0.2654\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1520\u001b[0m\n",
      "\u001b[94mLoss: 1.003527 | Gradient Norm: 0.2500\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1530\u001b[0m\n",
      "\u001b[94mLoss: 0.861964 | Gradient Norm: 0.1794\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1540\u001b[0m\n",
      "\u001b[94mLoss: 0.898051 | Gradient Norm: 0.1795\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1550\u001b[0m\n",
      "\u001b[94mLoss: 0.763492 | Gradient Norm: 0.1658\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1560\u001b[0m\n",
      "\u001b[94mLoss: 0.975859 | Gradient Norm: 0.1958\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1570\u001b[0m\n",
      "\u001b[94mLoss: 1.115626 | Gradient Norm: 0.2500\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1575.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8509.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8496.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1580\u001b[0m\n",
      "\u001b[94mLoss: 1.065126 | Gradient Norm: 0.1763\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1590\u001b[0m\n",
      "\u001b[94mLoss: 0.935852 | Gradient Norm: 0.1548\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1600\u001b[0m\n",
      "\u001b[94mLoss: 1.063007 | Gradient Norm: 0.2118\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1610\u001b[0m\n",
      "\u001b[94mLoss: 0.769428 | Gradient Norm: 0.1887\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1620\u001b[0m\n",
      "\u001b[94mLoss: 0.858046 | Gradient Norm: 0.2565\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1630\u001b[0m\n",
      "\u001b[94mLoss: 0.730908 | Gradient Norm: 0.3657\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1640\u001b[0m\n",
      "\u001b[94mLoss: 0.813878 | Gradient Norm: 0.1415\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1650\u001b[0m\n",
      "\u001b[94mLoss: 0.808661 | Gradient Norm: 0.1474\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1660\u001b[0m\n",
      "\u001b[94mLoss: 0.865159 | Gradient Norm: 0.1344\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1670\u001b[0m\n",
      "\u001b[94mLoss: 0.747251 | Gradient Norm: 0.1996\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1680\u001b[0m\n",
      "\u001b[94mLoss: 1.052692 | Gradient Norm: 0.2354\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1690\u001b[0m\n",
      "\u001b[94mLoss: 1.064582 | Gradient Norm: 0.3935\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1700\u001b[0m\n",
      "\u001b[94mLoss: 0.869714 | Gradient Norm: 0.2515\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1710\u001b[0m\n",
      "\u001b[94mLoss: 0.709084 | Gradient Norm: 0.1404\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1720\u001b[0m\n",
      "\u001b[94mLoss: 0.837978 | Gradient Norm: 0.1691\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1730\u001b[0m\n",
      "\u001b[94mLoss: 0.741588 | Gradient Norm: 0.1824\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1740\u001b[0m\n",
      "\u001b[94mLoss: 0.892900 | Gradient Norm: 0.1340\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1750.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8493.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8493. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-1750-2024-11-18-16-34-38.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1750\u001b[0m\n",
      "\u001b[94mLoss: 0.897427 | Gradient Norm: 0.2337\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1760\u001b[0m\n",
      "\u001b[94mLoss: 0.849020 | Gradient Norm: 0.1482\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1770\u001b[0m\n",
      "\u001b[94mLoss: 0.675465 | Gradient Norm: 0.1412\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1780\u001b[0m\n",
      "\u001b[94mLoss: 0.984943 | Gradient Norm: 0.2027\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1790\u001b[0m\n",
      "\u001b[94mLoss: 0.759850 | Gradient Norm: 0.1463\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1800\u001b[0m\n",
      "\u001b[94mLoss: 0.822230 | Gradient Norm: 0.1837\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1810\u001b[0m\n",
      "\u001b[94mLoss: 0.850420 | Gradient Norm: 0.1631\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1820\u001b[0m\n",
      "\u001b[94mLoss: 0.837826 | Gradient Norm: 0.2129\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1830\u001b[0m\n",
      "\u001b[94mLoss: 0.861722 | Gradient Norm: 0.1950\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1840\u001b[0m\n",
      "\u001b[94mLoss: 0.865134 | Gradient Norm: 0.1263\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1850\u001b[0m\n",
      "\u001b[94mLoss: 0.736227 | Gradient Norm: 0.1087\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1860\u001b[0m\n",
      "\u001b[94mLoss: 0.967109 | Gradient Norm: 0.1757\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1870\u001b[0m\n",
      "\u001b[94mLoss: 0.781815 | Gradient Norm: 0.1803\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1880\u001b[0m\n",
      "\u001b[94mLoss: 0.987135 | Gradient Norm: 0.3539\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1890\u001b[0m\n",
      "\u001b[94mLoss: 0.747212 | Gradient Norm: 0.1652\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1900\u001b[0m\n",
      "\u001b[94mLoss: 0.953805 | Gradient Norm: 0.1879\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1910\u001b[0m\n",
      "\u001b[94mLoss: 0.674635 | Gradient Norm: 0.1522\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1920\u001b[0m\n",
      "\u001b[94mLoss: 0.716023 | Gradient Norm: 0.2110\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1925.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8496.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8493.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1930\u001b[0m\n",
      "\u001b[94mLoss: 0.868967 | Gradient Norm: 0.1841\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1940\u001b[0m\n",
      "\u001b[94mLoss: 0.927696 | Gradient Norm: 0.2433\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1950\u001b[0m\n",
      "\u001b[94mLoss: 0.852604 | Gradient Norm: 0.1435\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1960\u001b[0m\n",
      "\u001b[94mLoss: 0.658573 | Gradient Norm: 0.1157\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1970\u001b[0m\n",
      "\u001b[94mLoss: 0.894167 | Gradient Norm: 0.1386\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1980\u001b[0m\n",
      "\u001b[94mLoss: 0.869310 | Gradient Norm: 0.2245\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1990\u001b[0m\n",
      "\u001b[94mLoss: 0.931768 | Gradient Norm: 0.1760\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2000\u001b[0m\n",
      "\u001b[94mLoss: 0.791098 | Gradient Norm: 0.1095\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2010\u001b[0m\n",
      "\u001b[94mLoss: 0.857177 | Gradient Norm: 0.3463\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2020\u001b[0m\n",
      "\u001b[94mLoss: 0.679104 | Gradient Norm: 0.2662\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2030\u001b[0m\n",
      "\u001b[94mLoss: 0.789571 | Gradient Norm: 0.3373\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2040\u001b[0m\n",
      "\u001b[94mLoss: 0.863005 | Gradient Norm: 0.1048\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2050\u001b[0m\n",
      "\u001b[94mLoss: 1.038426 | Gradient Norm: 0.2243\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2060\u001b[0m\n",
      "\u001b[94mLoss: 0.870005 | Gradient Norm: 0.2630\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2070\u001b[0m\n",
      "\u001b[94mLoss: 0.801400 | Gradient Norm: 0.1804\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2080\u001b[0m\n",
      "\u001b[94mLoss: 0.922164 | Gradient Norm: 0.1404\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2090\u001b[0m\n",
      "\u001b[94mLoss: 0.721064 | Gradient Norm: 0.2049\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2100.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8513.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8493.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2100\u001b[0m\n",
      "\u001b[94mLoss: 0.782284 | Gradient Norm: 0.1768\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2110\u001b[0m\n",
      "\u001b[94mLoss: 1.070905 | Gradient Norm: 0.2347\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2120\u001b[0m\n",
      "\u001b[94mLoss: 0.628121 | Gradient Norm: 0.2321\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2130\u001b[0m\n",
      "\u001b[94mLoss: 0.962677 | Gradient Norm: 0.1483\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2140\u001b[0m\n",
      "\u001b[94mLoss: 0.655598 | Gradient Norm: 0.2639\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2150\u001b[0m\n",
      "\u001b[94mLoss: 0.742113 | Gradient Norm: 0.1061\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2160\u001b[0m\n",
      "\u001b[94mLoss: 0.807239 | Gradient Norm: 0.1968\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2170\u001b[0m\n",
      "\u001b[94mLoss: 0.737454 | Gradient Norm: 0.1593\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2180\u001b[0m\n",
      "\u001b[94mLoss: 0.889524 | Gradient Norm: 0.2380\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2190\u001b[0m\n",
      "\u001b[94mLoss: 1.250316 | Gradient Norm: 0.2865\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2200\u001b[0m\n",
      "\u001b[94mLoss: 0.663640 | Gradient Norm: 0.2314\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2210\u001b[0m\n",
      "\u001b[94mLoss: 0.791361 | Gradient Norm: 0.2614\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2220\u001b[0m\n",
      "\u001b[94mLoss: 0.936239 | Gradient Norm: 0.2935\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2230\u001b[0m\n",
      "\u001b[94mLoss: 0.886522 | Gradient Norm: 0.1364\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2240\u001b[0m\n",
      "\u001b[94mLoss: 1.648386 | Gradient Norm: 0.3410\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2250\u001b[0m\n",
      "\u001b[94mLoss: 0.720741 | Gradient Norm: 0.1382\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2260\u001b[0m\n",
      "\u001b[94mLoss: 0.886876 | Gradient Norm: 0.1456\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2270\u001b[0m\n",
      "\u001b[94mLoss: 0.830902 | Gradient Norm: 0.1382\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2275.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8492.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8492. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-2275-2024-11-18-16-34-38.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2280\u001b[0m\n",
      "\u001b[94mLoss: 0.719386 | Gradient Norm: 0.2308\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2290\u001b[0m\n",
      "\u001b[94mLoss: 0.685409 | Gradient Norm: 0.3375\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2300\u001b[0m\n",
      "\u001b[94mLoss: 1.106950 | Gradient Norm: 0.2611\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2310\u001b[0m\n",
      "\u001b[94mLoss: 0.870160 | Gradient Norm: 0.2804\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2320\u001b[0m\n",
      "\u001b[94mLoss: 0.952470 | Gradient Norm: 0.2194\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2330\u001b[0m\n",
      "\u001b[94mLoss: 0.764269 | Gradient Norm: 0.2435\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2340\u001b[0m\n",
      "\u001b[94mLoss: 0.766579 | Gradient Norm: 0.1549\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2350\u001b[0m\n",
      "\u001b[94mLoss: 1.100925 | Gradient Norm: 0.2999\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2360\u001b[0m\n",
      "\u001b[94mLoss: 1.112220 | Gradient Norm: 0.2393\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2370\u001b[0m\n",
      "\u001b[94mLoss: 0.723149 | Gradient Norm: 0.1557\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2380\u001b[0m\n",
      "\u001b[94mLoss: 0.835865 | Gradient Norm: 0.2926\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2390\u001b[0m\n",
      "\u001b[94mLoss: 0.840678 | Gradient Norm: 0.1151\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2400\u001b[0m\n",
      "\u001b[94mLoss: 0.923906 | Gradient Norm: 0.1264\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2410\u001b[0m\n",
      "\u001b[94mLoss: 1.035478 | Gradient Norm: 0.2168\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2420\u001b[0m\n",
      "\u001b[94mLoss: 0.648766 | Gradient Norm: 0.2620\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2430\u001b[0m\n",
      "\u001b[94mLoss: 0.760006 | Gradient Norm: 0.1570\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2440\u001b[0m\n",
      "\u001b[94mLoss: 1.211867 | Gradient Norm: 0.3796\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2450.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8495.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8492.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2450\u001b[0m\n",
      "\u001b[94mLoss: 0.957097 | Gradient Norm: 0.1790\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2460\u001b[0m\n",
      "\u001b[94mLoss: 0.844759 | Gradient Norm: 0.1077\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2470\u001b[0m\n",
      "\u001b[94mLoss: 0.716886 | Gradient Norm: 0.2280\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2480\u001b[0m\n",
      "\u001b[94mLoss: 0.833740 | Gradient Norm: 0.2945\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2490\u001b[0m\n",
      "\u001b[94mLoss: 0.726993 | Gradient Norm: 0.1327\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2500\u001b[0m\n",
      "\u001b[94mLoss: 0.937494 | Gradient Norm: 0.1290\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2510\u001b[0m\n",
      "\u001b[94mLoss: 0.872840 | Gradient Norm: 0.2485\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2520\u001b[0m\n",
      "\u001b[94mLoss: 0.739128 | Gradient Norm: 0.2142\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2530\u001b[0m\n",
      "\u001b[94mLoss: 0.807271 | Gradient Norm: 0.1649\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2540\u001b[0m\n",
      "\u001b[94mLoss: 0.790754 | Gradient Norm: 0.1598\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2550\u001b[0m\n",
      "\u001b[94mLoss: 0.824840 | Gradient Norm: 0.2447\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2560\u001b[0m\n",
      "\u001b[94mLoss: 0.912820 | Gradient Norm: 0.1513\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2570\u001b[0m\n",
      "\u001b[94mLoss: 0.810070 | Gradient Norm: 0.3187\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2580\u001b[0m\n",
      "\u001b[94mLoss: 1.009753 | Gradient Norm: 0.2334\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2590\u001b[0m\n",
      "\u001b[94mLoss: 0.822978 | Gradient Norm: 0.1769\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2600\u001b[0m\n",
      "\u001b[94mLoss: 0.807002 | Gradient Norm: 0.1280\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2610\u001b[0m\n",
      "\u001b[94mLoss: 0.831598 | Gradient Norm: 0.2582\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2620\u001b[0m\n",
      "\u001b[94mLoss: 1.266328 | Gradient Norm: 0.3137\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2625.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8494.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8492.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2630\u001b[0m\n",
      "\u001b[94mLoss: 0.727337 | Gradient Norm: 0.1838\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2640\u001b[0m\n",
      "\u001b[94mLoss: 0.724446 | Gradient Norm: 0.3179\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2650\u001b[0m\n",
      "\u001b[94mLoss: 0.761687 | Gradient Norm: 0.1937\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2660\u001b[0m\n",
      "\u001b[94mLoss: 0.932004 | Gradient Norm: 0.2318\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2670\u001b[0m\n",
      "\u001b[94mLoss: 0.785060 | Gradient Norm: 0.2951\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2680\u001b[0m\n",
      "\u001b[94mLoss: 0.706427 | Gradient Norm: 0.1573\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2690\u001b[0m\n",
      "\u001b[94mLoss: 0.838903 | Gradient Norm: 0.1412\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2700\u001b[0m\n",
      "\u001b[94mLoss: 0.766169 | Gradient Norm: 0.1876\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2710\u001b[0m\n",
      "\u001b[94mLoss: 0.799422 | Gradient Norm: 0.1474\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2720\u001b[0m\n",
      "\u001b[94mLoss: 1.083196 | Gradient Norm: 0.3162\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2730\u001b[0m\n",
      "\u001b[94mLoss: 0.759173 | Gradient Norm: 0.1685\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2740\u001b[0m\n",
      "\u001b[94mLoss: 0.773079 | Gradient Norm: 0.2669\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2750\u001b[0m\n",
      "\u001b[94mLoss: 0.770167 | Gradient Norm: 0.0881\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2760\u001b[0m\n",
      "\u001b[94mLoss: 0.788770 | Gradient Norm: 0.1490\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2770\u001b[0m\n",
      "\u001b[94mLoss: 0.750015 | Gradient Norm: 0.1363\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2780\u001b[0m\n",
      "\u001b[94mLoss: 0.972287 | Gradient Norm: 0.4050\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2790\u001b[0m\n",
      "\u001b[94mLoss: 0.831940 | Gradient Norm: 0.1956\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2800.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8501.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8492.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2800\u001b[0m\n",
      "\u001b[94mLoss: 0.644185 | Gradient Norm: 0.1460\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2810.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8495.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8492.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2810\u001b[0m\n",
      "\u001b[94mLoss: 0.849088 | Gradient Norm: 0.2586\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "for epoch in range(num_epochs):\n",
    "    for step, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs,  targets = inputs.to(device), targets.to(device)\n",
    "        relative_step = epoch * steps_per_epoch + step\n",
    "        last_step = relative_step == num_steps - 1\n",
    "        # print(inputs.shape,  targets.shape)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss,  metrics = loss_fn(outputs, targets)  # Assuming `loss_function` is defined\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip global norm of gradient at 1.0, per the GPT-3 paper\n",
    "        grad_norms.append(grad_norm.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        # Periodically evaluate the model on validation set\n",
    "        if relative_step % (eval_period // dilation) == 0 or last_step:\n",
    "            colored_print(\n",
    "                f\"\\nEvaluating the spectral SSM model at step {relative_step}.\",\n",
    "                Colors.OKCYAN,\n",
    "            )\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss, metrics = loss_fn(val_outputs, val_targets) \n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= len(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                val_time_steps.append(relative_step)\n",
    "            model.train()\n",
    "\n",
    "            colored_print(\n",
    "                f\"\\nValidation Loss: {val_loss:.4f}.\",\n",
    "                Colors.OKCYAN,\n",
    "            )\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_step = relative_step\n",
    "                patient_counter = 0\n",
    "\n",
    "                # Save model and optimizer state\n",
    "                model_checkpoint = f\"sssm-{controller}-model_step-{relative_step}-{timestamp}.pt\"\n",
    "               \n",
    "                model_path = os.path.join(checkpoint_dir, model_checkpoint)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "                extra_info = f\"sssm-{controller}-other_step-{relative_step}-{timestamp}.pt\"\n",
    "                best_checkpoint =model_checkpoint, extra_info\n",
    "                extra_info_path = os.path.join(checkpoint_dir, extra_info)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"configs\": model.configs,\n",
    "                        \"step\": relative_step,\n",
    "                        \"val_loss\": val_loss,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"rng_state_pytorch\": torch.get_rng_state(),\n",
    "                        \"rng_state_numpy\": np.random.get_state(),\n",
    "                        \"rng_state_cuda\": torch.cuda.get_rng_state_all()\n",
    "                        if torch.cuda.is_available()\n",
    "                        else None,\n",
    "                    },\n",
    "                    extra_info_path,\n",
    "                )\n",
    "                colored_print(\n",
    "                    f\"New best validation loss: {val_loss:.4f}. Model checkpoint saved at {model_path}.\",\n",
    "                    Colors.OKGREEN,\n",
    "                )\n",
    "            else:\n",
    "                patient_counter += 1\n",
    "                colored_print(\n",
    "                    f\"No improvement in validation loss. Current best: {best_val_loss:.4f}.\",\n",
    "                    Colors.WARNING,\n",
    "                )\n",
    "\n",
    "            if patient_counter >= patience:\n",
    "                colored_print(\n",
    "                    f\"Stopping early due to patience limit of {patience}.\",\n",
    "                    Colors.FAIL,\n",
    "                )\n",
    "                break\n",
    "\n",
    "        # Logging\n",
    "        if relative_step % 10 == 0:\n",
    "            colored_print(f\"\\nStep {relative_step:5d}\", Colors.HEADER)\n",
    "            colored_print(f\"Loss: {loss.item():.6f} | Gradient Norm: {grad_norm:.4f}\", Colors.OKBLUE)\n",
    "\n",
    "    if patient_counter >= patience:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_42600\\1417480683.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n",
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_42600\\1417480683.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  other_data = torch.load(best_model_extra_info_path, map_location=\"cpu\")\n",
      "c:\\Users\\devan\\OneDrive\\Documents\\mujoco\\sssm_basic\\utils\\loss_landscape.py:124: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=self.dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model information for the spectral SSM model:\n",
      "    Best model at step 2275\n",
      "    Best validation loss: 0.8492\n",
      "    Best model checkpoint saved at: checkpoints\\sssm-Walker2D-v1-model_step-2275-2024-11-18-16-34-38.pt\n",
      "    Additional data saved at: checkpoints\\sssm-Walker2D-v1-other_step-2275-2024-11-18-16-34-38.pt\n",
      "Training details saved in training_details_sssm_2024-11-18-16-34-38.txt. Congratulations!\n",
      "Data saved to results\\mujoco-v1\\sssm\\sssm-Walker2D-v1-train_losses-2024-11-18-16-34-38.txt\n",
      "Data saved to results\\mujoco-v1\\sssm\\sssm-Walker2D-v1-val_losses-2024-11-18-16-34-38.txt\n",
      "Data saved to results\\mujoco-v1\\sssm\\sssm-Walker2D-v1-val_time_steps-2024-11-18-16-34-38.txt\n",
      "Data saved to results\\mujoco-v1\\sssm\\sssm-Walker2D-v1-grad_norms-2024-11-18-16-34-38.txt\n",
      "Training run completed. Results have been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\OneDrive\\Documents\\mujoco\\sssm_basic\\utils\\loss_landscape.py:351: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_data = torch.load(landscape_data)\n",
      "c:\\Users\\devan\\OneDrive\\Documents\\mujoco\\sssm_basic\\utils\\loss_landscape.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_data = torch.load(landscape_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mVTS file with grid saved to landscapes/loss_landscape-2024-11-18-16-34-38.vts\u001b[0m\n",
      "\u001b[92mLoss landscape visualization saved to landscapes/loss_landscape-2024-11-18-16-34-38.png\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devan\\OneDrive\\Documents\\mujoco\\sssm_basic\\utils\\loss_landscape.py:635: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_data = torch.load(landscape_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mHessian eigenvalue ratio heatmap saved to landscapes/loss_landscape-2024-11-18-16-34-38_hessian_heatmap.png\u001b[0m\n",
      "Percentage of convex regions: 100.00%\n",
      "Percentage of near-convex regions (ratio >= -0.01): 100.00%\n"
     ]
    }
   ],
   "source": [
    "from  train import save_results\n",
    "task = 'mujoco-v1'\n",
    "\n",
    "if best_checkpoint:\n",
    "    model_checkpoint, extra_info = best_checkpoint\n",
    "    best_model_path = os.path.join(checkpoint_dir, model_checkpoint)\n",
    "    best_model_extra_info_path = os.path.join(checkpoint_dir, extra_info)\n",
    "\n",
    "    # Load the best checkpoint\n",
    "    state_dict = torch.load(best_model_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Load optimizer and other data\n",
    "    other_data = torch.load(best_model_extra_info_path, map_location=\"cpu\")\n",
    "    optimizer.load_state_dict(other_data[\"optimizer\"])\n",
    "\n",
    "    # Display best model information\n",
    "    print(\"\\nBest model information for the spectral SSM model:\")\n",
    "    print(f\"    Best model at step {best_model_step}\")\n",
    "    print(f\"    Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"    Best model checkpoint saved at: {best_model_path}\")\n",
    "    print(f\"    Additional data saved at: {best_model_extra_info_path}\")\n",
    "\n",
    "    # Save training details to a file\n",
    "    training_details_path = f\"training_details_sssm_{timestamp}.txt\"\n",
    "    with open(training_details_path, \"w\") as f:\n",
    "        f.write(f\"Training completed for the spectral SSM on {task} at: {datetime.now()}\\n\")\n",
    "        f.write(f\"Best model step: {best_model_step}\\n\")\n",
    "        f.write(f\"Best validation loss: {best_val_loss:.4f}\\n\")\n",
    "        f.write(f\"Best model checkpoint saved at: {best_model_path}\\n\")\n",
    "        f.write(f\"Additional data saved at: {best_model_extra_info_path}\\n\")\n",
    "\n",
    "    print(f\"Training details saved in {training_details_path}. Congratulations!\")\n",
    "else:\n",
    "    print(\"\\nNo best checkpoint found. The model did not improve during training.\")\n",
    "\n",
    "# Save final results\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "if not os.path.exists(\"results/\"):\n",
    "    os.makedirs(\"results/\")\n",
    "if not os.path.exists(\"landscapes\"):\n",
    "    os.makedirs(\"landscapes\", exist_ok=True)\n",
    "\n",
    "save_results(task, controller, train_losses, \"train_losses\", timestamp)\n",
    "save_results(task, controller,val_losses, \"val_losses\", timestamp)\n",
    "save_results(task, controller, val_time_steps,\"val_time_steps\", timestamp)\n",
    "save_results(task, controller, grad_norms, \"grad_norms\", timestamp)\n",
    "\n",
    "print(\"Training run completed. Results have been saved.\")\n",
    "\n",
    "# from utils.loss_landscape import LossLandscape (takes 20 minutes to run)\n",
    "# #Don't exactly know how to visualize thi\n",
    "# loss_landscape = LossLandscape(model, device, optimizer, lr)\n",
    "# x_range = (-1, 1, 10)  # Adjust as needed\n",
    "# y_range = (-1, 1, 10)\n",
    "# loss_landscape.generate(\n",
    "#     train_loader,\n",
    "#     f\"landscapes/loss_landscape-{timestamp}\",\n",
    "#     x_range=x_range,\n",
    "#     y_range=y_range,\n",
    "#     plot_loss_landscape=True,\n",
    "#     plot_hessian=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSSM(\n",
       "  (loss_fn): MSELoss()\n",
       "  (emb): Linear(in_features=24, out_features=8, bias=False)\n",
       "  (stu): ModuleDict(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (hidden): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (rn_1): RMSNorm((8,), eps=None, elementwise_affine=True)\n",
       "        (stu): STU(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (rn_2): RMSNorm((8,), eps=None, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (swiglu): SwiGLU(\n",
       "            (w): Linear(in_features=8, out_features=32, bias=False)\n",
       "            (v): Linear(in_features=8, out_features=32, bias=False)\n",
       "            (w2): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (task_head): Linear(in_features=8, out_features=18, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
