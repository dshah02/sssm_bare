{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PM-OpidNr-2t"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VD6FmmptNgi",
    "outputId": "7ce67015-39d1-45d4-eacf-eddb26355488"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bfe36213b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_IXCcwxLytwu"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') #required for imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xq-hPW8IzSOg"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The SwiGLU activation function,\n",
    "from \"GLU Variants Improve Transformer\" (Shazeer, 2020).\n",
    "\n",
    "From the paper:\n",
    "'We offer no explanation as to why these architectures seem to work;\n",
    "we attribute their success, as all else, to __divine benevolence__.'\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    The SwiGLU activation function as proposed by Noam Shazeer.\n",
    "\n",
    "    This module implements the SwiGLU function defined as:\n",
    "    FFN_SwiGLU(x, W, V, W2) = (Swish_{1}(xW) ⊙ (xV))W2\n",
    "    where ⊙ denotes the Hadamard product and Swish_{1} is the Swish function with β=1.\n",
    "\n",
    "    Note: The Swish function with β=1 is equivalent to PyTorch's SiLU function.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Input and output dimension.\n",
    "        h_dim (int): Hidden dimension.\n",
    "        bias (bool, optional): If false, additive biases will not be learned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, h_dim, bias=False):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(dim, h_dim, bias=bias)\n",
    "        self.v = nn.Linear(dim, h_dim, bias=bias)\n",
    "        self.w2 = nn.Linear(h_dim, dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w(x)) * self.v(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "a-i6Ahw0uV1d"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def get_hankel(n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates a Hankel matrix Z, as defined in Equation (3) of the paper.\n",
    "\n",
    "    This special matrix is used for the spectral filtering in the Spectral\n",
    "    Transform Unit (STU).\n",
    "\n",
    "    Args:\n",
    "        n (int): Size of the square Hankel matrix.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Hankel matrix Z of shape [n, n].\n",
    "    \"\"\"\n",
    "    i = torch.arange(1, n + 1)\n",
    "    s = i[:, None] + i[None, :]\n",
    "    Z = 2.0 / (s**3 - s)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mlmAFn2_uaft"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def get_top_eigh(\n",
    "    n: int, K: int, use_hankel_L: bool, device: torch.device\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns the top K eigenvalues and eigenvectors of the Hankel matrix Z.\n",
    "\n",
    "    These eigenvalues and eigenvectors are used to construct the spectral\n",
    "    filters for the STU model, as described in Section 3 of the paper.\n",
    "\n",
    "    Args:\n",
    "        n (int): Size of the Hankel matrix.\n",
    "        K (int): Number of top eigenvalues/eigenvectors to return.\n",
    "        use_hankel_L (bool): If True, use the alternative Hankel matrix Z_L.\n",
    "        device (torch.device): Computation device (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]:\n",
    "            - sigma: Top K eigenvalues [K]\n",
    "            - phi: The corresponding eigenvectors [n, K]\n",
    "    \"\"\"\n",
    "    # Z = get_hankel_L(n).to(device) if use_hankel_L else get_hankel(n).to(device)\n",
    "    Z = get_hankel(n).to(device)\n",
    "    sigma, phi = torch.linalg.eigh(Z)\n",
    "    return sigma[-K:], phi[:, -K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vBUguWnnsI1z"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def shift(u: torch.Tensor, k: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Rolls the time axis forward by k steps to align the input u_{t-k} with u_t.\n",
    "    This effectively removes the last k time steps of the input tensor.\n",
    "\n",
    "    This function implements the time shifting functionality needed for\n",
    "    the autoregressive component in Equation 4 of the STU model (Section 3).\n",
    "\n",
    "    Args:\n",
    "        u (torch.Tensor): An input tensor of shape [bsz, sl, K, d].\n",
    "        k (int): Number of time steps to shift. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Shifted tensor of shape [bsz, sl, K, d].\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return u\n",
    "    shifted = torch.roll(u, shifts=k, dims=1)\n",
    "    shifted[:, :k] = 0\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0kjpNlHKsD3Q"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def compute_ar_u(M_u: torch.Tensor, u: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the autoregressive component of the STU model with respect to\n",
    "    the input, as described in Equation (4) of Section 3.\n",
    "\n",
    "    This function implements the sum of M^u_i u_{t+1-i} from i=1 to\n",
    "    (more generally) k_u (in the paper, it was up until i=3).\n",
    "\n",
    "    Args:\n",
    "        M_u (torch.Tensor): Input weight matrices of shape (d_out, k_u, d_in)\n",
    "        u (torch.Tensor): Input tensor of shape (bsz, sl, d_in)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Autoregressive component w.r.t. input of shape (bsz, sl, d_out)\n",
    "    \"\"\"\n",
    "    k_u = M_u.shape[1]\n",
    "\n",
    "    # Sum M^u_i \\hat_{u}_{t+1-i} from i=1 to i=k_u\n",
    "    u_shifted = torch.stack([shift(u, i) for i in range(k_u)], dim=1)\n",
    "    ar_u = torch.einsum(\"bksd,dki->bsi\", u_shifted, M_u)\n",
    "\n",
    "    return ar_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xxwcxaYex3-K"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def compute_ar_y(M_y: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the autoregressive component of the AR-STU model with respect to\n",
    "    the output, as described in Equation (6) of Section 5.\n",
    "\n",
    "    This function implements the sum of M^y_i y_{t-i} from i=1 to i=k_y.\n",
    "\n",
    "    Args:\n",
    "        M_y: Output weight matrices of shape (d_out, k_y, d_out)\n",
    "        y: Predictions (bsz, sl, d_out)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Autoregressive component w.r.t. output of shape (bsz, sl, d_out)\n",
    "    \"\"\"\n",
    "    # k_y = M_y.shape[0]\n",
    "\n",
    "    # # Sum M^y_i \\hat_{y}_{t-i} from i=1 to i=k_y\n",
    "    # y_shifted = torch.stack([shift(y, i + 1) for i in range(k_y)], dim=1)\n",
    "    # ar_y = torch.einsum(\"bksd,kod->bso\", y_shifted, M_y)\n",
    "    # return ar_y\n",
    "    k, d_out, _ = M_y.shape\n",
    "    bsz, sl, _ = y.shape\n",
    "\n",
    "    # Initialize carry buffer and output tensor\n",
    "    carry = torch.zeros((bsz, k, d_out), device=y.device)\n",
    "    ys = torch.zeros((bsz, sl, d_out), device=y.device)\n",
    "\n",
    "    # Process each timestep\n",
    "    for t in range(sl):\n",
    "        # Current input: (bsz, d_out)\n",
    "        x = y[:, t]\n",
    "\n",
    "        # Compute AR component: (bsz, d_out)\n",
    "        output = torch.einsum('dko,bkd->bo', M_y, carry) + x\n",
    "\n",
    "        # Store output\n",
    "        ys[:, t] = output\n",
    "\n",
    "        # Update carry buffer\n",
    "        carry = torch.cat([output.unsqueeze(1), carry[:, :-1]], dim=1)\n",
    "\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "3qBxujdSAyAG"
   },
   "outputs": [],
   "source": [
    "def compute_ar(M_y, M_u, u):\n",
    "    \"\"\"\n",
    "    Computes both autoregressive components of the STU model\n",
    "    as described in Equation (4) of Section 3.\n",
    "\n",
    "    Args:\n",
    "        M_y (torch.Tensor): Input weight matrices of shape (d_out, k_y, d_in)\n",
    "        M_u (torch.Tensor): Input weight matrices of shape (d_out, k_u, d_in)\n",
    "        u (torch.Tensor): Input tensor of shape (B, L, d_in)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Autoregressive component w.r.t. input of shape (B, L, d_out)\n",
    "    \"\"\"\n",
    "    bsz, seq_len, _ = u.shape\n",
    "    D, k_y, _ = M_y.shape\n",
    "    output_list = []  # Store outputs for each timestep\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        terms = []  # Collect all terms for this timestep\n",
    "\n",
    "        # AR component\n",
    "        for i in range(1, k_y + 1):\n",
    "            if t - i >= 0:\n",
    "                M_i = M_y[:, i-1]\n",
    "                y_prev = output_list[t-i]\n",
    "                terms.append(y_prev @ M_i.T)\n",
    "\n",
    "        # Control component\n",
    "        if M_u is not None:\n",
    "            k_u = M_u.shape[1]\n",
    "            for i in range(1, k_u + 1):\n",
    "                if t + 1 - i >= 0 and t + 1 - i < seq_len:\n",
    "                    u_prev = u[:, t+1-i]\n",
    "                    terms.append(u_prev @ M_u[:, i-1].T)\n",
    "\n",
    "        # Sum all terms for this timestep\n",
    "        if terms:\n",
    "            output_list.append(sum(terms))\n",
    "        else:\n",
    "            output_list.append(torch.zeros(bsz, D, device=u.device))\n",
    "\n",
    "    # Stack all timesteps\n",
    "    return torch.stack(output_list, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOQUSSFMq7lZ",
    "outputId": "cdb5f2f4-3eb9-4f3a-8762-6dc05d672b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def compute_ar_y(M_y: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the autoregressive component of the AR-STU model.\n",
    "\n",
    "    Args:\n",
    "        M_y: Output weight matrices of shape (d_out, k_y, d_out)\n",
    "        y: Predictions (bsz, sl, d_out)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Autoregressive component (bsz, sl, d_out)\n",
    "    \"\"\"\n",
    "    k, d_out, _ = M_y.shape\n",
    "    bsz, sl, _ = y.shape\n",
    "\n",
    "    # Initialize carry buffer and output tensor\n",
    "    carry = torch.zeros((bsz, k, d_out), device=y.device)\n",
    "    ys = torch.zeros((bsz, sl, d_out), device=y.device)\n",
    "\n",
    "    # Process each timestep\n",
    "    for t in range(sl):\n",
    "        # Current input: (bsz, d_out)\n",
    "        x = y[:, t]\n",
    "\n",
    "        # Compute AR component: (bsz, d_out)\n",
    "        output = torch.einsum('kdo,bkd->bo', M_y, carry) + x\n",
    "\n",
    "        # Store output\n",
    "        ys[:, t] = output\n",
    "\n",
    "        # Update carry buffer\n",
    "        carry = torch.cat([output.unsqueeze(1), carry[:, :-1]], dim=1)\n",
    "\n",
    "    return ys\n",
    "\n",
    "def test_compute_ar_y():\n",
    "    \"\"\"Test the autoregressive computation\"\"\"\n",
    "    k_y, d_out = 2, 3\n",
    "    bsz, sl = 4, 5\n",
    "\n",
    "    # Test shapes\n",
    "    M_y = torch.randn(k_y, d_out, d_out)\n",
    "    y = torch.randn(bsz, sl, d_out)\n",
    "    output = compute_ar_y(M_y, y)\n",
    "    assert output.shape == (bsz, sl, d_out)\n",
    "\n",
    "    # Test first timestep (should just be input)\n",
    "    assert torch.allclose(output[:, 0], y[:, 0])\n",
    "\n",
    "    # Test with identity matrices\n",
    "    M_y = torch.eye(d_out).unsqueeze(0).repeat(k_y, 1, 1)\n",
    "    y = torch.ones(bsz, sl, d_out)\n",
    "    output = compute_ar_y(M_y, y)\n",
    "    assert output.shape == (bsz, sl, d_out)\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run tests\n",
    "test_compute_ar_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "aopSEWFCx5Zw"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def nearest_power_of_2(x: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns the smallest power of 2 that is greater than or equal to x.\n",
    "    If x is already a power of 2, it returns x itself.\n",
    "    Otherwise, it returns the next higher power of 2.\n",
    "\n",
    "    Args:\n",
    "        x (int): The input integer.\n",
    "\n",
    "    Returns:\n",
    "        int: The smallest power of 2 that is greater than or equal to x.\n",
    "    \"\"\"\n",
    "    s = bin(x)\n",
    "    s = s.lstrip(\"-0b\")\n",
    "    length = len(s)\n",
    "    return 1 << (length - 1) if x == 1 << (length - 1) else 1 << length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "UwsOdhjux8Q4"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def conv(u: torch.Tensor, phi: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Implements the FFT convolution of the input sequences into the Hankel\n",
    "    spectral basis, as described in Section 3 of the paper.\n",
    "\n",
    "    This function computes U⁺_{t,k} and U⁻_{t,k}, which are the positive and\n",
    "    negative featurizations of the input sequence, respectively.\n",
    "\n",
    "    Args:\n",
    "        u (torch.Tensor): Input of shape [bsz, sl, d].\n",
    "        phi (torch.Tensor): Top K eigenvectors of shape [sl, K].\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Feature tensors U⁺ and U⁻ of shape [bsz, sl, K, d].\n",
    "    \"\"\"\n",
    "    bsz, sl, d = u.shape\n",
    "    _, K = phi.shape\n",
    "\n",
    "    # Round sequence length to the nearest power of 2 for efficient convolution\n",
    "    n = nearest_power_of_2(sl * 2 - 1)\n",
    "\n",
    "    # Add bsz and d dims to phi and u and expand to the return shape\n",
    "    phi = phi.view(1, -1, K, 1).expand(bsz, -1, K, d)\n",
    "    u = u.view(bsz, -1, 1, d).expand(bsz, -1, K, d)\n",
    "\n",
    "    # Compute U⁺\n",
    "    V = torch.fft.rfft(phi, n=n, dim=1)\n",
    "    U = torch.fft.rfft(u, n=n, dim=1)\n",
    "    U_plus = torch.fft.irfft(V * U, n=n, dim=1)[:, :sl]\n",
    "\n",
    "    # Generate alternating signs tensor, (-1)^i of length sl, match dims of u\n",
    "    alt = torch.ones(sl, device=u.device)\n",
    "    alt[1::2] = -1  # Replace every other element with -1, starting from index 1\n",
    "    alt = alt.view(1, sl, 1, 1).expand_as(u)\n",
    "\n",
    "    # Compute U⁻\n",
    "    u_alt = u * alt\n",
    "    U_alt = torch.fft.rfft(u_alt, n=n, dim=1)\n",
    "    U_minus = torch.fft.irfft(V * U_alt, n=n, dim=1)[:, :sl]\n",
    "\n",
    "    return U_plus, U_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "D1OPt-Dex-P1"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def compute_spectral(\n",
    "    inputs: torch.Tensor,\n",
    "    eigh: tuple[torch.Tensor, torch.Tensor],\n",
    "    M_phi_plus: torch.Tensor,\n",
    "    M_phi_minus: torch.Tensor,\n",
    "    M_y: torch.Tensor = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the spectral component of the STU or AR-STU model, as described\n",
    "    in Equations (4) and (6) of the paper.\n",
    "\n",
    "    This function projects the input tensor into the spectral basis via\n",
    "    convolution and applies the precomputed spectral filters.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): A tensor of shape [bsz, sl, d_in].\n",
    "        eigh (tuple[torch.Tensor, torch.Tensor]): Eigenvalues [K,] and eigenvectors [sl, K].\n",
    "        M_phi_plus (torch.Tensor): Positive spectral filter weights [d_out, K, d_in].\n",
    "        M_phi_minus (torch.Tensor): Negative spectral filter weights [d_out, K, d_in].\n",
    "        M_y (torch.Tensor, optional): Autoregressive weights for AR-STU [d_out, k_y, d_out].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The spectral component tensor of shape [bsz, sl, d_out].\n",
    "    \"\"\"\n",
    "    sigma, phi = eigh\n",
    "    _, K = phi.shape\n",
    "\n",
    "    # Compute U⁺ and U⁻\n",
    "    U_plus, U_minus = conv(inputs, phi)  # -> tuple of [bsz, sl, K, d_in]\n",
    "\n",
    "    # Shift U⁺ and U⁻ k_y time steps\n",
    "    if M_y is not None:\n",
    "        k_y = M_y.shape[1]\n",
    "        U_plus, U_minus = shift(U_plus, k_y), shift(U_minus, k_y)\n",
    "\n",
    "    # Perform spectral filter on U⁺ and U⁻ w/ sigma\n",
    "    sigma_root = (sigma**0.25).view(1, 1, K, 1)\n",
    "    U_plus_filtered, U_minus_filtered = U_plus * sigma_root, U_minus * sigma_root\n",
    "\n",
    "    # Sum M^{\\phi +}_k \\cdot U_plus_filtered across K filters\n",
    "    spectral_plus = torch.einsum(\"bsKd,dKo->bso\", U_plus_filtered, M_phi_plus)\n",
    "\n",
    "    # Sum M^{\\phi -}_k \\cdot U_minus_filtered across K filters\n",
    "    spectral_minus = torch.einsum(\"bsKd,dKo->bso\", U_minus_filtered, M_phi_minus)\n",
    "\n",
    "    return spectral_plus + spectral_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "H7w9I0sKyZVf"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SSSMConfigs:\n",
    "    d_in: int = 24\n",
    "    d_out: int = 18\n",
    "    n_layers: int = 6\n",
    "    n_embd: int = 512\n",
    "    sl: int = 300\n",
    "    scale: int = 4\n",
    "    bias: bool = False\n",
    "    dropout: float = 0.10\n",
    "    num_eigh: int = 24\n",
    "    k_u: int = 3  # Number of parametrizable, autoregressive matrices Mᵘ\n",
    "    k_y: int = 2  # Number of parametrizable, autoregressive matrices Mʸ\n",
    "    learnable_m_y: bool = True\n",
    "    alpha: float = 0.9  # 0.9 deemed \"uniformly optimal\" in the paper\n",
    "    use_hankel_L: bool = False\n",
    "    loss_fn: nn.Module = nn.MSELoss()\n",
    "    controls: dict = field(\n",
    "        default_factory=lambda: {\"task\": \"mujoco-v3\", \"controller\": \"Ant-v1\"}\n",
    "    )\n",
    "\n",
    "\n",
    "class STU(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple STU (Spectral Transform Unit) layer.\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object containing the following attributes:\n",
    "            d_in (int): Input dimension.\n",
    "            d_out (int): Output dimension.\n",
    "            sl (int): Input sequence length.\n",
    "            num_eigh (int): Number of spectral filters to use.\n",
    "            k_u (int): Autoregressive depth on the input sequence.\n",
    "            k_y (int): Autoregressive depth on the output sequence.\n",
    "            use_hankel_L (bool): Use the alternative Hankel matrix?\n",
    "            learnable_m_y (bool): Learn the M_y matrix?\n",
    "            dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs) -> None:\n",
    "        super(STU, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.d_in = configs.n_embd\n",
    "        self.d_out = configs.n_embd\n",
    "        self.l, self.k = configs.sl, configs.num_eigh\n",
    "        self.use_hankel_L = configs.use_hankel_L\n",
    "        self.eigh = get_top_eigh(self.l, self.k, self.use_hankel_L, self.device)\n",
    "        self.k_u = configs.k_u\n",
    "        self.k_y = configs.k_y\n",
    "        self.learnable_m_y = configs.learnable_m_y\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        # Parameterizable matrix Mᵘ, Mᵠ⁺, and Mᵠ⁻, per section 3\n",
    "        self.M_u = nn.Parameter(torch.empty(self.d_out, self.k_u, self.d_in))\n",
    "        self.M_phi_plus = nn.Parameter(torch.empty(self.d_out, self.k, self.d_in))\n",
    "        self.M_phi_minus = nn.Parameter(torch.empty(self.d_out, self.k, self.d_in))\n",
    "\n",
    "        # Parametrizable matrix Mʸ Introduced in section 5, equation 5\n",
    "        if self.learnable_m_y:\n",
    "            self.M_y = nn.Parameter(torch.zeros(self.d_out, self.k_y, self.d_out))\n",
    "        else:\n",
    "            self.register_buffer(\"M_y\", torch.zeros(self.d_out, self.k_y, self.d_out))\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the STU layer.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor of shape (bsz, sl, d_in)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (bsz, sl, d_out)\n",
    "        \"\"\"\n",
    "        y_t = compute_spectral(inputs, self.eigh, self.M_phi_plus, self.M_phi_minus, self.M_y)\n",
    "\n",
    "        if self.k_u > 0:\n",
    "          if self.k_y > 0:\n",
    "            y_t += compute_ar(self.M_y, self.M_u, inputs)\n",
    "          else:\n",
    "            y_t += compute_ar_u(self.M_u, inputs)\n",
    "\n",
    "        return self.dropout(y_t)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple multi-layer perceptron network using SwiGLU activation.\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object containing the following attributes:\n",
    "            scale (float): Scaling factor for hidden dimension.\n",
    "            n_embd (int): Embedding dimension.\n",
    "            bias (bool): Whether to use bias in linear layers.\n",
    "            dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs: SSSMConfigs) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.h_dim = configs.scale * configs.n_embd\n",
    "        self.swiglu = SwiGLU(dim=configs.n_embd, h_dim=self.h_dim, bias=configs.bias)\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the MLP.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.swiglu(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single block of the SSSM model, consisting of STU and MLP layers.\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object for STU and MLP layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs: SSSMConfigs) -> None:\n",
    "        super(Block, self).__init__()\n",
    "        # Good configuration\n",
    "        self.rn_1 = nn.RMSNorm(configs.n_embd)\n",
    "        self.stu = STU(configs)\n",
    "        self.rn_2 = nn.RMSNorm(configs.n_embd)\n",
    "        self.mlp = MLP(configs)\n",
    "\n",
    "        # Basic configuration\n",
    "        # self.stu = STU(configs) #this also has a size problem [d_in,d_out] instead of [d_emb,  d_emb]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        # Good configuration\n",
    "        x = x + self.stu(self.rn_1(x))\n",
    "        x = x + self.mlp(self.rn_2(x))\n",
    "\n",
    "        # Basic configuration (no skips, no non-linearities)\n",
    "        # x = self.stu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    General model architecture based on stacked STU blocks and MLP layers.\n",
    "\n",
    "    Args:\n",
    "        configs: Configuration object containing model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs: SSSMConfigs) -> None:\n",
    "        super(SSSM, self).__init__()\n",
    "        self.configs = configs\n",
    "        self.n_layers = configs.n_layers\n",
    "        self.n_embd = configs.n_embd\n",
    "        self.d_in = configs.d_in\n",
    "        self.d_out = configs.d_out\n",
    "        self.sl = configs.sl\n",
    "        self.learnable_m_y = configs.learnable_m_y\n",
    "        self.alpha = configs.alpha\n",
    "\n",
    "        self.bias = configs.bias\n",
    "        self.dropout = configs.dropout\n",
    "        self.loss_fn = configs.loss_fn\n",
    "        self.controls = configs.controls\n",
    "\n",
    "        self.emb = nn.Linear(self.d_in, self.n_embd, bias=self.bias)\n",
    "        self.stu = nn.ModuleDict(\n",
    "            dict(\n",
    "                dropout=nn.Dropout(self.dropout),\n",
    "                hidden=nn.ModuleList([Block(configs) for _ in range(self.n_layers)]),\n",
    "            )\n",
    "        )\n",
    "        self.task_head = nn.Linear(self.n_embd, self.d_out, bias=self.bias)\n",
    "\n",
    "        # Initialize all weights\n",
    "        self.m_x = self.d_out**-0.5\n",
    "        self.std = self.n_embd**-0.5\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Report the number of parameters\n",
    "        print(\"STU Model Parameter Count: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "\n",
    "    def forward(self, inputs, targets = None):\n",
    "        \"\"\"\n",
    "        Forward pass of the SSSM model.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor of shape (bsz, sl, d_in)\n",
    "            targets (torch.Tensor): Target tensor for loss computation\n",
    "\n",
    "        Returns:\n",
    "            Type (ignore due to high variability):\n",
    "            - Predictions tensor\n",
    "            - Tuple containing loss and metrics (if applicable)\n",
    "        \"\"\"\n",
    "        # _, sl, n_embd = inputs.size()\n",
    "\n",
    "        x = self.emb(inputs)\n",
    "        x = self.stu.dropout(x)\n",
    "\n",
    "        for block in self.stu.hidden:\n",
    "            x = block(x)\n",
    "\n",
    "        preds = self.task_head(x)\n",
    "\n",
    "        loss = self.loss_fn(preds, targets) if targets is not None else None\n",
    "        return ((preds, loss) if loss is not None else preds)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initialize the weights of the model.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): The module to initialize.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            self.std *= (2 * self.n_layers) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.std)\n",
    "        elif isinstance(module, STU):\n",
    "            torch.nn.init.uniform_(module.M_u, -self.m_x, self.m_x)\n",
    "            torch.nn.init.zeros_(module.M_phi_plus)\n",
    "            torch.nn.init.zeros_(module.M_phi_minus)\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "\n",
    "        Args:\n",
    "            non_embedding (bool, optional):\n",
    "            Whether to exclude the positional embeddings (if applicable).\n",
    "            Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        return num_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8RJEm4Z0hQ_",
    "outputId": "cd7bcf1b-1cb4-4e81-ac4e-7e36bdd095ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STU Model Parameter Count: 0.01M\n"
     ]
    }
   ],
   "source": [
    "bsz=1 # @param\n",
    "n_layers=2 # @param\n",
    "n_embd=8 # @param\n",
    "d_in=24 # @param\n",
    "d_out=18 # @param\n",
    "sl=512 # @param\n",
    "scale=4 # @param\n",
    "bias=False # @param\n",
    "dropout=0.0 # @param\n",
    "num_eigh=16 # @param\n",
    "k_u=3 # @param\n",
    "k_y=0 # @param\n",
    "learnable_m_y=True # @param\n",
    "alpha=0.9 # @param\n",
    "use_hankel_L=False # @param\n",
    "loss_fn=nn.MSELoss() # @param\n",
    "lr=1e-1 # @param\n",
    "delta=0.01 # @param\n",
    "\n",
    "configs = SSSMConfigs(\n",
    "  n_layers=n_layers,\n",
    "  n_embd=n_embd,\n",
    "  d_in=d_in,\n",
    "  d_out=d_out,\n",
    "  sl=sl,\n",
    "  scale=scale,\n",
    "  bias=bias,\n",
    "  dropout=dropout,\n",
    "  num_eigh=num_eigh,\n",
    "  k_u=k_u,\n",
    "  k_y=k_y,\n",
    "  learnable_m_y=learnable_m_y,\n",
    "  alpha=alpha,\n",
    "  use_hankel_L=use_hankel_L,\n",
    "  loss_fn=loss_fn,\n",
    ")\n",
    "\n",
    "model = SSSM(configs).to(device)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m\n",
      "Creating dataloader on cuda for task: mujoco-v1\u001b[0m\n",
      "Using sequence length: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apply Gaussian noise to data?: Disabled\n",
      "\u001b[94m\n",
      "Calculating data statistics...\u001b[0m\n",
      "\u001b[94mNormalizing data...\u001b[0m\n",
      "\u001b[94mValidating data normalization...\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for coordinates: [0.00018377 0.0004239 ]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for coordinates: [0.99988493 1.00014359]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for coordinates: [-0.00018377 -0.0004239 ]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for coordinates: [1.00011158 0.99980374]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for angles: [-0.00020589 -0.00096165 -0.00018324  0.00073418 -0.00010059 -0.00011929\n",
      " -0.00011729]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for angles: [1.00041433 1.00185566 1.00013121 1.00021316 1.00018604 1.00035338\n",
      " 1.00017155]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for angles: [ 0.00020589  0.00096165  0.00018324 -0.00073418  0.00010059  0.00011929\n",
      "  0.00011729]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for angles: [0.99954492 0.9977162  0.9997683  0.9997608  0.99939967 0.99954043\n",
      " 0.9998055 ]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for coordinate_velocities: [ 0.00023379 -0.00012828]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for coordinate_velocities: [1.00050389 1.00003277]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for coordinate_velocities: [-0.00023379  0.00012828]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for coordinate_velocities: [0.99948129 0.99995449]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for angular_velocities: [ 3.28207562e-05  2.49162060e-05 -1.21271749e-05 -3.17272072e-04\n",
      "  1.78593172e-05 -5.56011537e-06  2.38656492e-04]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for angular_velocities: [1.00014932 1.00054625 1.00016789 1.00032316 0.99962519 1.00011136\n",
      " 0.99996271]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for angular_velocities: [-3.28207562e-05 -2.49162060e-05  1.21271749e-05  3.17272072e-04\n",
      " -1.78593172e-05  5.56011537e-06 -2.38656492e-04]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for angular_velocities: [0.99984485 0.99944538 0.99982847 0.99967394 1.00036605 0.99988489\n",
      " 1.00003486]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for torque: [-4.62952337e-17 -4.98681692e-17 -5.88959294e-18 -6.86438376e-16\n",
      " -8.99262482e-17  5.87139334e-17]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for torque: [0.99997337 0.99997864 0.99998755 0.99977441 0.99997176 0.99998938]\u001b[0m\n",
      "\u001b[1m\u001b[92mData normalization validated successfully.\u001b[0m\n",
      "\u001b[92mDataloader created successfully.\u001b[0m\n",
      "\u001b[94m\n",
      "Creating dataloader on cuda for task: mujoco-v1\u001b[0m\n",
      "Using sequence length: 512\n",
      "\n",
      "Apply Gaussian noise to data?: Disabled\n",
      "\u001b[94m\n",
      "Calculating data statistics...\u001b[0m\n",
      "\u001b[94mNormalizing data...\u001b[0m\n",
      "\u001b[94mValidating data normalization...\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for coordinates: [-0.00073541 -0.00169415]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for coordinates: [1.00045294 0.99929953]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for coordinates: [0.00073541 0.00169415]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for coordinates: [0.99954287 1.00064472]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for angles: [ 0.00082333  0.00383343  0.00073323 -0.00293492  0.00040419  0.00047672\n",
      "  0.00046875]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for angles: [0.99824278 0.99149925 0.99922457 0.99908021 0.99821533 0.99831713\n",
      " 0.99925645]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for angles: [-0.00082333 -0.00383343 -0.00073323  0.00293492 -0.00040419 -0.00047672\n",
      " -0.00046875]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for angles: [1.00171304 1.0079956  1.00067394 1.00088491 1.00136749 1.00157388\n",
      " 1.00071988]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for coordinate_velocities: [-0.0009364   0.00051173]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for coordinate_velocities: [0.99793967 0.99983744]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for coordinate_velocities: [ 0.0009364  -0.00051173]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for coordinate_velocities: [1.00204072 1.00014959]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for angular_velocities: [-1.31102936e-04 -9.94288561e-05  4.84091778e-05  1.26707851e-03\n",
      " -7.12301366e-05  2.21757815e-05 -9.53706499e-04]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for angular_velocities: [0.99938935 0.99780371 0.99932105 0.99870094 1.00146769 0.99954576\n",
      " 1.00014297]\u001b[0m\n",
      "\u001b[92mNormalized mean of targets for angular_velocities: [ 1.31102936e-04  9.94288561e-05 -4.84091778e-05 -1.26707851e-03\n",
      "  7.12301366e-05 -2.21757815e-05  9.53706499e-04]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of targets for angular_velocities: [1.00060446 1.00218343 1.00067489 1.00129309 0.99852153 1.00045031\n",
      " 0.99985372]\u001b[0m\n",
      "\u001b[92m\n",
      "Normalized mean of inputs for torque: [ 4.51998167e-17  1.74249741e-17 -2.87451428e-17 -6.49171249e-16\n",
      " -6.95988233e-17  3.67905485e-17]\u001b[0m\n",
      "\u001b[92mNormalized standard deviation of inputs for torque: [0.99997357 0.99997865 0.99998755 0.99977556 0.99997179 0.99998939]\u001b[0m\n",
      "\u001b[1m\u001b[92mData normalization validated successfully.\u001b[0m\n",
      "\u001b[92mDataloader created successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "controller = 'Walker2D-v1' #@param\n",
    "train_data = {\n",
    "    \"inputs\": f\"data/mujoco-v1/{controller}/train_inputs.npy\",\n",
    "    \"targets\": f\"data/mujoco-v1/{controller}/train_targets.npy\",\n",
    "}\n",
    "val_data = {\n",
    "    \"inputs\": f\"data/mujoco-v1/{controller}/val_inputs.npy\",\n",
    "    \"targets\": f\"data/mujoco-v1/{controller}/val_targets.npy\",\n",
    "}\n",
    "\n",
    "from utils.dataloader import get_dataloader \n",
    "\n",
    "train_loader = get_dataloader(\n",
    "    model  = 'spectral_ssm',\n",
    "    data = train_data,\n",
    "    task = 'mujoco-v1',\n",
    "    controller = controller,\n",
    "    bsz  = bsz,\n",
    "    preprocess =  True, #normalize\n",
    "    shuffle= True,\n",
    "    sl = sl,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "val_loader = get_dataloader(\n",
    "    model  = 'spectral_ssm',\n",
    "    data = val_data,\n",
    "    task = 'mujoco-v1',\n",
    "    controller =  controller,\n",
    "    bsz = bsz,\n",
    "    preprocess =  True, #normalize\n",
    "    shuffle= True,\n",
    "    sl = sl,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using batch size: 1\n",
      "Number of epochs: 1\n",
      "Steps per epoch: 2811\n",
      "=> Number of training steps: 2811\n"
     ]
    }
   ],
   "source": [
    "training_stu = True #@param\n",
    "num_epochs: int = 1 #@param\n",
    "steps_per_epoch = len(train_loader) #@param\n",
    "num_steps: int = steps_per_epoch * num_epochs #@param\n",
    "dilation: int = 1 #@param\n",
    "warmup_steps: int = num_steps // 8 #@param\n",
    "eval_period: int = num_steps // 16 #@param\n",
    "\n",
    "print(f\"\\nUsing batch size: {bsz}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"=> Number of training steps: {num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from utils.colors import Colors, colored_print\n",
    "from losses.loss_ant import AntLoss\n",
    "from losses.loss_cheetah import HalfCheetahLoss\n",
    "from losses.loss_walker import Walker2DLoss\n",
    "from losses.loss_cartpole import CartpoleLoss\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "checkpoint_dir: str = \"checkpoints\"\n",
    "if controller == \"Ant-v1\":\n",
    "    loss_fn = AntLoss()\n",
    "elif controller == \"HalfCheetah-v1\":\n",
    "    loss_fn = HalfCheetahLoss()\n",
    "elif controller == \"Walker2D-v1\":\n",
    "    loss_fn = Walker2DLoss()\n",
    "elif controller == \"CartPole-v1\":\n",
    "    loss_fn = CartpoleLoss()\n",
    "else:\n",
    "    loss_fn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_counter = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_step = 0\n",
    "best_checkpoint = None\n",
    "patience: int = 10 #number of  non-improving eval periods before early stoppig\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_time_steps = []\n",
    "grad_norms = []\n",
    "\n",
    "model  = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 0.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 1.9683.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 1.9683. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-0-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step     0\u001b[0m\n",
      "\u001b[94mLoss: 1.971432 | Gradient Norm: 0.6307\u001b[0m\n",
      "\u001b[95m\n",
      "Step    10\u001b[0m\n",
      "\u001b[94mLoss: 1.039928 | Gradient Norm: 0.5656\u001b[0m\n",
      "\u001b[95m\n",
      "Step    20\u001b[0m\n",
      "\u001b[94mLoss: 0.819690 | Gradient Norm: 0.3232\u001b[0m\n",
      "\u001b[95m\n",
      "Step    30\u001b[0m\n",
      "\u001b[94mLoss: 1.256488 | Gradient Norm: 0.4011\u001b[0m\n",
      "\u001b[95m\n",
      "Step    40\u001b[0m\n",
      "\u001b[94mLoss: 0.801146 | Gradient Norm: 0.2998\u001b[0m\n",
      "\u001b[95m\n",
      "Step    50\u001b[0m\n",
      "\u001b[94mLoss: 0.758886 | Gradient Norm: 0.1969\u001b[0m\n",
      "\u001b[95m\n",
      "Step    60\u001b[0m\n",
      "\u001b[94mLoss: 0.851063 | Gradient Norm: 0.2753\u001b[0m\n",
      "\u001b[95m\n",
      "Step    70\u001b[0m\n",
      "\u001b[94mLoss: 0.748405 | Gradient Norm: 0.1743\u001b[0m\n",
      "\u001b[95m\n",
      "Step    80\u001b[0m\n",
      "\u001b[94mLoss: 0.895121 | Gradient Norm: 0.2710\u001b[0m\n",
      "\u001b[95m\n",
      "Step    90\u001b[0m\n",
      "\u001b[94mLoss: 0.830962 | Gradient Norm: 0.5261\u001b[0m\n",
      "\u001b[95m\n",
      "Step   100\u001b[0m\n",
      "\u001b[94mLoss: 1.261117 | Gradient Norm: 0.3106\u001b[0m\n",
      "\u001b[95m\n",
      "Step   110\u001b[0m\n",
      "\u001b[94mLoss: 0.870344 | Gradient Norm: 0.3866\u001b[0m\n",
      "\u001b[95m\n",
      "Step   120\u001b[0m\n",
      "\u001b[94mLoss: 0.833179 | Gradient Norm: 0.2418\u001b[0m\n",
      "\u001b[95m\n",
      "Step   130\u001b[0m\n",
      "\u001b[94mLoss: 0.778811 | Gradient Norm: 0.1900\u001b[0m\n",
      "\u001b[95m\n",
      "Step   140\u001b[0m\n",
      "\u001b[94mLoss: 0.713457 | Gradient Norm: 0.1561\u001b[0m\n",
      "\u001b[95m\n",
      "Step   150\u001b[0m\n",
      "\u001b[94mLoss: 0.779014 | Gradient Norm: 0.2204\u001b[0m\n",
      "\u001b[95m\n",
      "Step   160\u001b[0m\n",
      "\u001b[94mLoss: 0.749896 | Gradient Norm: 0.2369\u001b[0m\n",
      "\u001b[95m\n",
      "Step   170\u001b[0m\n",
      "\u001b[94mLoss: 0.849785 | Gradient Norm: 0.2848\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 175.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8700.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8700. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-175-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   180\u001b[0m\n",
      "\u001b[94mLoss: 0.770905 | Gradient Norm: 0.1775\u001b[0m\n",
      "\u001b[95m\n",
      "Step   190\u001b[0m\n",
      "\u001b[94mLoss: 1.248054 | Gradient Norm: 0.2907\u001b[0m\n",
      "\u001b[95m\n",
      "Step   200\u001b[0m\n",
      "\u001b[94mLoss: 0.807797 | Gradient Norm: 0.1729\u001b[0m\n",
      "\u001b[95m\n",
      "Step   210\u001b[0m\n",
      "\u001b[94mLoss: 0.707271 | Gradient Norm: 0.1582\u001b[0m\n",
      "\u001b[95m\n",
      "Step   220\u001b[0m\n",
      "\u001b[94mLoss: 0.957021 | Gradient Norm: 0.3009\u001b[0m\n",
      "\u001b[95m\n",
      "Step   230\u001b[0m\n",
      "\u001b[94mLoss: 0.821289 | Gradient Norm: 0.2584\u001b[0m\n",
      "\u001b[95m\n",
      "Step   240\u001b[0m\n",
      "\u001b[94mLoss: 0.814751 | Gradient Norm: 0.2388\u001b[0m\n",
      "\u001b[95m\n",
      "Step   250\u001b[0m\n",
      "\u001b[94mLoss: 1.486725 | Gradient Norm: 0.4743\u001b[0m\n",
      "\u001b[95m\n",
      "Step   260\u001b[0m\n",
      "\u001b[94mLoss: 0.886244 | Gradient Norm: 0.1782\u001b[0m\n",
      "\u001b[95m\n",
      "Step   270\u001b[0m\n",
      "\u001b[94mLoss: 1.087457 | Gradient Norm: 0.2466\u001b[0m\n",
      "\u001b[95m\n",
      "Step   280\u001b[0m\n",
      "\u001b[94mLoss: 0.778180 | Gradient Norm: 0.1654\u001b[0m\n",
      "\u001b[95m\n",
      "Step   290\u001b[0m\n",
      "\u001b[94mLoss: 0.761257 | Gradient Norm: 0.2136\u001b[0m\n",
      "\u001b[95m\n",
      "Step   300\u001b[0m\n",
      "\u001b[94mLoss: 0.862364 | Gradient Norm: 0.2644\u001b[0m\n",
      "\u001b[95m\n",
      "Step   310\u001b[0m\n",
      "\u001b[94mLoss: 2.298979 | Gradient Norm: 0.3754\u001b[0m\n",
      "\u001b[95m\n",
      "Step   320\u001b[0m\n",
      "\u001b[94mLoss: 0.846479 | Gradient Norm: 0.1485\u001b[0m\n",
      "\u001b[95m\n",
      "Step   330\u001b[0m\n",
      "\u001b[94mLoss: 0.773747 | Gradient Norm: 0.3536\u001b[0m\n",
      "\u001b[95m\n",
      "Step   340\u001b[0m\n",
      "\u001b[94mLoss: 0.892880 | Gradient Norm: 0.1836\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 350.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8677.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8677. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-350-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   350\u001b[0m\n",
      "\u001b[94mLoss: 0.670310 | Gradient Norm: 0.2256\u001b[0m\n",
      "\u001b[95m\n",
      "Step   360\u001b[0m\n",
      "\u001b[94mLoss: 0.698878 | Gradient Norm: 0.2340\u001b[0m\n",
      "\u001b[95m\n",
      "Step   370\u001b[0m\n",
      "\u001b[94mLoss: 0.958036 | Gradient Norm: 0.2571\u001b[0m\n",
      "\u001b[95m\n",
      "Step   380\u001b[0m\n",
      "\u001b[94mLoss: 1.283095 | Gradient Norm: 0.3183\u001b[0m\n",
      "\u001b[95m\n",
      "Step   390\u001b[0m\n",
      "\u001b[94mLoss: 0.952013 | Gradient Norm: 0.1754\u001b[0m\n",
      "\u001b[95m\n",
      "Step   400\u001b[0m\n",
      "\u001b[94mLoss: 0.877295 | Gradient Norm: 0.1714\u001b[0m\n",
      "\u001b[95m\n",
      "Step   410\u001b[0m\n",
      "\u001b[94mLoss: 0.886054 | Gradient Norm: 0.1512\u001b[0m\n",
      "\u001b[95m\n",
      "Step   420\u001b[0m\n",
      "\u001b[94mLoss: 0.895116 | Gradient Norm: 0.1871\u001b[0m\n",
      "\u001b[95m\n",
      "Step   430\u001b[0m\n",
      "\u001b[94mLoss: 0.842533 | Gradient Norm: 0.1160\u001b[0m\n",
      "\u001b[95m\n",
      "Step   440\u001b[0m\n",
      "\u001b[94mLoss: 0.828019 | Gradient Norm: 0.1353\u001b[0m\n",
      "\u001b[95m\n",
      "Step   450\u001b[0m\n",
      "\u001b[94mLoss: 0.756766 | Gradient Norm: 0.2568\u001b[0m\n",
      "\u001b[95m\n",
      "Step   460\u001b[0m\n",
      "\u001b[94mLoss: 0.888007 | Gradient Norm: 0.2242\u001b[0m\n",
      "\u001b[95m\n",
      "Step   470\u001b[0m\n",
      "\u001b[94mLoss: 0.817798 | Gradient Norm: 0.1969\u001b[0m\n",
      "\u001b[95m\n",
      "Step   480\u001b[0m\n",
      "\u001b[94mLoss: 0.900760 | Gradient Norm: 0.3235\u001b[0m\n",
      "\u001b[95m\n",
      "Step   490\u001b[0m\n",
      "\u001b[94mLoss: 0.851752 | Gradient Norm: 0.1713\u001b[0m\n",
      "\u001b[95m\n",
      "Step   500\u001b[0m\n",
      "\u001b[94mLoss: 0.919647 | Gradient Norm: 0.1859\u001b[0m\n",
      "\u001b[95m\n",
      "Step   510\u001b[0m\n",
      "\u001b[94mLoss: 0.835312 | Gradient Norm: 0.2459\u001b[0m\n",
      "\u001b[95m\n",
      "Step   520\u001b[0m\n",
      "\u001b[94mLoss: 0.827410 | Gradient Norm: 0.1707\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 525.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8661.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8661. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-525-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   530\u001b[0m\n",
      "\u001b[94mLoss: 0.848008 | Gradient Norm: 0.3171\u001b[0m\n",
      "\u001b[95m\n",
      "Step   540\u001b[0m\n",
      "\u001b[94mLoss: 1.078777 | Gradient Norm: 0.2513\u001b[0m\n",
      "\u001b[95m\n",
      "Step   550\u001b[0m\n",
      "\u001b[94mLoss: 0.706150 | Gradient Norm: 0.1699\u001b[0m\n",
      "\u001b[95m\n",
      "Step   560\u001b[0m\n",
      "\u001b[94mLoss: 0.828427 | Gradient Norm: 0.2285\u001b[0m\n",
      "\u001b[95m\n",
      "Step   570\u001b[0m\n",
      "\u001b[94mLoss: 0.878075 | Gradient Norm: 0.1316\u001b[0m\n",
      "\u001b[95m\n",
      "Step   580\u001b[0m\n",
      "\u001b[94mLoss: 0.761963 | Gradient Norm: 0.2147\u001b[0m\n",
      "\u001b[95m\n",
      "Step   590\u001b[0m\n",
      "\u001b[94mLoss: 0.670998 | Gradient Norm: 0.1237\u001b[0m\n",
      "\u001b[95m\n",
      "Step   600\u001b[0m\n",
      "\u001b[94mLoss: 0.893938 | Gradient Norm: 0.3923\u001b[0m\n",
      "\u001b[95m\n",
      "Step   610\u001b[0m\n",
      "\u001b[94mLoss: 0.841345 | Gradient Norm: 0.1507\u001b[0m\n",
      "\u001b[95m\n",
      "Step   620\u001b[0m\n",
      "\u001b[94mLoss: 0.870120 | Gradient Norm: 0.3545\u001b[0m\n",
      "\u001b[95m\n",
      "Step   630\u001b[0m\n",
      "\u001b[94mLoss: 0.861607 | Gradient Norm: 0.2028\u001b[0m\n",
      "\u001b[95m\n",
      "Step   640\u001b[0m\n",
      "\u001b[94mLoss: 0.811591 | Gradient Norm: 0.2239\u001b[0m\n",
      "\u001b[95m\n",
      "Step   650\u001b[0m\n",
      "\u001b[94mLoss: 1.034750 | Gradient Norm: 0.2598\u001b[0m\n",
      "\u001b[95m\n",
      "Step   660\u001b[0m\n",
      "\u001b[94mLoss: 1.229245 | Gradient Norm: 0.3032\u001b[0m\n",
      "\u001b[95m\n",
      "Step   670\u001b[0m\n",
      "\u001b[94mLoss: 1.137807 | Gradient Norm: 0.2837\u001b[0m\n",
      "\u001b[95m\n",
      "Step   680\u001b[0m\n",
      "\u001b[94mLoss: 0.659953 | Gradient Norm: 0.1178\u001b[0m\n",
      "\u001b[95m\n",
      "Step   690\u001b[0m\n",
      "\u001b[94mLoss: 0.737290 | Gradient Norm: 0.1070\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 700.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8643.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8643. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-700-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   700\u001b[0m\n",
      "\u001b[94mLoss: 0.883492 | Gradient Norm: 0.1396\u001b[0m\n",
      "\u001b[95m\n",
      "Step   710\u001b[0m\n",
      "\u001b[94mLoss: 0.783752 | Gradient Norm: 0.1488\u001b[0m\n",
      "\u001b[95m\n",
      "Step   720\u001b[0m\n",
      "\u001b[94mLoss: 0.875455 | Gradient Norm: 0.1959\u001b[0m\n",
      "\u001b[95m\n",
      "Step   730\u001b[0m\n",
      "\u001b[94mLoss: 0.943575 | Gradient Norm: 0.2472\u001b[0m\n",
      "\u001b[95m\n",
      "Step   740\u001b[0m\n",
      "\u001b[94mLoss: 0.921210 | Gradient Norm: 0.2131\u001b[0m\n",
      "\u001b[95m\n",
      "Step   750\u001b[0m\n",
      "\u001b[94mLoss: 0.803463 | Gradient Norm: 0.1614\u001b[0m\n",
      "\u001b[95m\n",
      "Step   760\u001b[0m\n",
      "\u001b[94mLoss: 0.985694 | Gradient Norm: 0.1561\u001b[0m\n",
      "\u001b[95m\n",
      "Step   770\u001b[0m\n",
      "\u001b[94mLoss: 0.688319 | Gradient Norm: 0.2071\u001b[0m\n",
      "\u001b[95m\n",
      "Step   780\u001b[0m\n",
      "\u001b[94mLoss: 0.827985 | Gradient Norm: 0.1312\u001b[0m\n",
      "\u001b[95m\n",
      "Step   790\u001b[0m\n",
      "\u001b[94mLoss: 0.801967 | Gradient Norm: 0.1369\u001b[0m\n",
      "\u001b[95m\n",
      "Step   800\u001b[0m\n",
      "\u001b[94mLoss: 0.847401 | Gradient Norm: 0.1658\u001b[0m\n",
      "\u001b[95m\n",
      "Step   810\u001b[0m\n",
      "\u001b[94mLoss: 1.226196 | Gradient Norm: 0.2404\u001b[0m\n",
      "\u001b[95m\n",
      "Step   820\u001b[0m\n",
      "\u001b[94mLoss: 0.935809 | Gradient Norm: 0.1574\u001b[0m\n",
      "\u001b[95m\n",
      "Step   830\u001b[0m\n",
      "\u001b[94mLoss: 0.834272 | Gradient Norm: 0.1611\u001b[0m\n",
      "\u001b[95m\n",
      "Step   840\u001b[0m\n",
      "\u001b[94mLoss: 0.843315 | Gradient Norm: 0.1524\u001b[0m\n",
      "\u001b[95m\n",
      "Step   850\u001b[0m\n",
      "\u001b[94mLoss: 0.804396 | Gradient Norm: 0.1853\u001b[0m\n",
      "\u001b[95m\n",
      "Step   860\u001b[0m\n",
      "\u001b[94mLoss: 0.784868 | Gradient Norm: 0.1209\u001b[0m\n",
      "\u001b[95m\n",
      "Step   870\u001b[0m\n",
      "\u001b[94mLoss: 0.752939 | Gradient Norm: 0.2517\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 875.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8670.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8643.\u001b[0m\n",
      "\u001b[95m\n",
      "Step   880\u001b[0m\n",
      "\u001b[94mLoss: 0.721411 | Gradient Norm: 0.2230\u001b[0m\n",
      "\u001b[95m\n",
      "Step   890\u001b[0m\n",
      "\u001b[94mLoss: 0.802880 | Gradient Norm: 0.1657\u001b[0m\n",
      "\u001b[95m\n",
      "Step   900\u001b[0m\n",
      "\u001b[94mLoss: 0.830803 | Gradient Norm: 0.2275\u001b[0m\n",
      "\u001b[95m\n",
      "Step   910\u001b[0m\n",
      "\u001b[94mLoss: 0.824452 | Gradient Norm: 0.2103\u001b[0m\n",
      "\u001b[95m\n",
      "Step   920\u001b[0m\n",
      "\u001b[94mLoss: 0.932204 | Gradient Norm: 0.2063\u001b[0m\n",
      "\u001b[95m\n",
      "Step   930\u001b[0m\n",
      "\u001b[94mLoss: 0.810554 | Gradient Norm: 0.2110\u001b[0m\n",
      "\u001b[95m\n",
      "Step   940\u001b[0m\n",
      "\u001b[94mLoss: 1.525360 | Gradient Norm: 0.3350\u001b[0m\n",
      "\u001b[95m\n",
      "Step   950\u001b[0m\n",
      "\u001b[94mLoss: 0.910502 | Gradient Norm: 0.1637\u001b[0m\n",
      "\u001b[95m\n",
      "Step   960\u001b[0m\n",
      "\u001b[94mLoss: 0.779976 | Gradient Norm: 0.2825\u001b[0m\n",
      "\u001b[95m\n",
      "Step   970\u001b[0m\n",
      "\u001b[94mLoss: 0.767182 | Gradient Norm: 0.1378\u001b[0m\n",
      "\u001b[95m\n",
      "Step   980\u001b[0m\n",
      "\u001b[94mLoss: 0.831719 | Gradient Norm: 0.3687\u001b[0m\n",
      "\u001b[95m\n",
      "Step   990\u001b[0m\n",
      "\u001b[94mLoss: 0.755188 | Gradient Norm: 0.1661\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1000\u001b[0m\n",
      "\u001b[94mLoss: 0.740466 | Gradient Norm: 0.1400\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1010\u001b[0m\n",
      "\u001b[94mLoss: 0.652970 | Gradient Norm: 0.1617\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1020\u001b[0m\n",
      "\u001b[94mLoss: 0.991600 | Gradient Norm: 0.1892\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1030\u001b[0m\n",
      "\u001b[94mLoss: 0.970351 | Gradient Norm: 0.1911\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1040\u001b[0m\n",
      "\u001b[94mLoss: 0.997767 | Gradient Norm: 0.2079\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1050.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8601.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8601. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-1050-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1050\u001b[0m\n",
      "\u001b[94mLoss: 0.701144 | Gradient Norm: 0.1618\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1060\u001b[0m\n",
      "\u001b[94mLoss: 0.904952 | Gradient Norm: 0.1151\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1070\u001b[0m\n",
      "\u001b[94mLoss: 0.803943 | Gradient Norm: 0.1468\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1080\u001b[0m\n",
      "\u001b[94mLoss: 0.789435 | Gradient Norm: 0.1660\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1090\u001b[0m\n",
      "\u001b[94mLoss: 0.795611 | Gradient Norm: 0.2244\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1100\u001b[0m\n",
      "\u001b[94mLoss: 0.952228 | Gradient Norm: 0.2688\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1110\u001b[0m\n",
      "\u001b[94mLoss: 0.820159 | Gradient Norm: 0.1111\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1120\u001b[0m\n",
      "\u001b[94mLoss: 0.820838 | Gradient Norm: 0.1234\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1130\u001b[0m\n",
      "\u001b[94mLoss: 1.089042 | Gradient Norm: 0.1948\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1140\u001b[0m\n",
      "\u001b[94mLoss: 0.983395 | Gradient Norm: 0.1546\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1150\u001b[0m\n",
      "\u001b[94mLoss: 0.828262 | Gradient Norm: 0.1370\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1160\u001b[0m\n",
      "\u001b[94mLoss: 0.870158 | Gradient Norm: 0.2529\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1170\u001b[0m\n",
      "\u001b[94mLoss: 1.071174 | Gradient Norm: 0.2895\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1180\u001b[0m\n",
      "\u001b[94mLoss: 0.864782 | Gradient Norm: 0.1560\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1190\u001b[0m\n",
      "\u001b[94mLoss: 0.925685 | Gradient Norm: 0.1696\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1200\u001b[0m\n",
      "\u001b[94mLoss: 0.814100 | Gradient Norm: 0.1575\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1210\u001b[0m\n",
      "\u001b[94mLoss: 0.783587 | Gradient Norm: 0.1536\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1220\u001b[0m\n",
      "\u001b[94mLoss: 0.935165 | Gradient Norm: 0.2144\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1225.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8608.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8601.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1230\u001b[0m\n",
      "\u001b[94mLoss: 0.557815 | Gradient Norm: 0.2834\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1240\u001b[0m\n",
      "\u001b[94mLoss: 0.750243 | Gradient Norm: 0.3195\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1250\u001b[0m\n",
      "\u001b[94mLoss: 0.921702 | Gradient Norm: 0.1798\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1260\u001b[0m\n",
      "\u001b[94mLoss: 0.774638 | Gradient Norm: 0.1408\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1270\u001b[0m\n",
      "\u001b[94mLoss: 0.728264 | Gradient Norm: 0.1301\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1280\u001b[0m\n",
      "\u001b[94mLoss: 0.868046 | Gradient Norm: 0.1664\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1290\u001b[0m\n",
      "\u001b[94mLoss: 0.962057 | Gradient Norm: 0.2411\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1300\u001b[0m\n",
      "\u001b[94mLoss: 0.778736 | Gradient Norm: 0.2310\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1310\u001b[0m\n",
      "\u001b[94mLoss: 0.850742 | Gradient Norm: 0.1318\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1320\u001b[0m\n",
      "\u001b[94mLoss: 0.801619 | Gradient Norm: 0.2024\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1330\u001b[0m\n",
      "\u001b[94mLoss: 1.038490 | Gradient Norm: 0.2950\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1340\u001b[0m\n",
      "\u001b[94mLoss: 0.716769 | Gradient Norm: 0.1359\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1350\u001b[0m\n",
      "\u001b[94mLoss: 1.042919 | Gradient Norm: 0.2065\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1360\u001b[0m\n",
      "\u001b[94mLoss: 0.726173 | Gradient Norm: 0.1646\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1370\u001b[0m\n",
      "\u001b[94mLoss: 0.672434 | Gradient Norm: 0.2500\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1380\u001b[0m\n",
      "\u001b[94mLoss: 0.853092 | Gradient Norm: 0.2342\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1390\u001b[0m\n",
      "\u001b[94mLoss: 0.845091 | Gradient Norm: 0.2248\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1400.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8593.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8593. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-1400-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1400\u001b[0m\n",
      "\u001b[94mLoss: 0.871837 | Gradient Norm: 0.1780\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1410\u001b[0m\n",
      "\u001b[94mLoss: 0.905923 | Gradient Norm: 0.2024\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1420\u001b[0m\n",
      "\u001b[94mLoss: 0.958295 | Gradient Norm: 0.1666\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1430\u001b[0m\n",
      "\u001b[94mLoss: 0.779502 | Gradient Norm: 0.2297\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1440\u001b[0m\n",
      "\u001b[94mLoss: 0.868337 | Gradient Norm: 0.1182\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1450\u001b[0m\n",
      "\u001b[94mLoss: 0.891753 | Gradient Norm: 0.1756\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1460\u001b[0m\n",
      "\u001b[94mLoss: 0.813784 | Gradient Norm: 0.1410\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1470\u001b[0m\n",
      "\u001b[94mLoss: 0.794324 | Gradient Norm: 0.2369\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1480\u001b[0m\n",
      "\u001b[94mLoss: 0.664472 | Gradient Norm: 0.2004\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1490\u001b[0m\n",
      "\u001b[94mLoss: 0.755878 | Gradient Norm: 0.1291\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1500\u001b[0m\n",
      "\u001b[94mLoss: 0.932838 | Gradient Norm: 0.3022\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1510\u001b[0m\n",
      "\u001b[94mLoss: 0.757419 | Gradient Norm: 0.1386\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1520\u001b[0m\n",
      "\u001b[94mLoss: 0.743136 | Gradient Norm: 0.0904\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1530\u001b[0m\n",
      "\u001b[94mLoss: 0.795186 | Gradient Norm: 0.1737\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1540\u001b[0m\n",
      "\u001b[94mLoss: 0.918293 | Gradient Norm: 0.1569\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1550\u001b[0m\n",
      "\u001b[94mLoss: 0.902503 | Gradient Norm: 0.1364\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1560\u001b[0m\n",
      "\u001b[94mLoss: 0.812767 | Gradient Norm: 0.1402\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1570\u001b[0m\n",
      "\u001b[94mLoss: 0.860353 | Gradient Norm: 0.2132\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1575.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8606.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8593.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1580\u001b[0m\n",
      "\u001b[94mLoss: 0.914776 | Gradient Norm: 0.1581\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1590\u001b[0m\n",
      "\u001b[94mLoss: 0.822441 | Gradient Norm: 0.1784\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1600\u001b[0m\n",
      "\u001b[94mLoss: 0.764081 | Gradient Norm: 0.1427\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1610\u001b[0m\n",
      "\u001b[94mLoss: 0.701544 | Gradient Norm: 0.1225\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1620\u001b[0m\n",
      "\u001b[94mLoss: 0.841368 | Gradient Norm: 0.1993\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1630\u001b[0m\n",
      "\u001b[94mLoss: 1.299281 | Gradient Norm: 0.3311\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1640\u001b[0m\n",
      "\u001b[94mLoss: 0.749238 | Gradient Norm: 0.1061\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1650\u001b[0m\n",
      "\u001b[94mLoss: 0.908798 | Gradient Norm: 0.1554\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1660\u001b[0m\n",
      "\u001b[94mLoss: 0.890324 | Gradient Norm: 0.1621\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1670\u001b[0m\n",
      "\u001b[94mLoss: 0.829417 | Gradient Norm: 0.1572\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1680\u001b[0m\n",
      "\u001b[94mLoss: 0.841194 | Gradient Norm: 0.1320\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1690\u001b[0m\n",
      "\u001b[94mLoss: 0.722179 | Gradient Norm: 0.1394\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1700\u001b[0m\n",
      "\u001b[94mLoss: 0.708852 | Gradient Norm: 0.2040\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1710\u001b[0m\n",
      "\u001b[94mLoss: 0.780687 | Gradient Norm: 0.2416\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1720\u001b[0m\n",
      "\u001b[94mLoss: 1.097464 | Gradient Norm: 0.2155\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1730\u001b[0m\n",
      "\u001b[94mLoss: 0.806574 | Gradient Norm: 0.1503\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1740\u001b[0m\n",
      "\u001b[94mLoss: 0.736876 | Gradient Norm: 0.1700\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1750.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8595.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8593.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1750\u001b[0m\n",
      "\u001b[94mLoss: 0.765238 | Gradient Norm: 0.2362\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1760\u001b[0m\n",
      "\u001b[94mLoss: 0.898767 | Gradient Norm: 0.1980\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1770\u001b[0m\n",
      "\u001b[94mLoss: 0.791896 | Gradient Norm: 0.1733\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1780\u001b[0m\n",
      "\u001b[94mLoss: 0.865136 | Gradient Norm: 0.1525\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1790\u001b[0m\n",
      "\u001b[94mLoss: 0.744209 | Gradient Norm: 0.2771\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1800\u001b[0m\n",
      "\u001b[94mLoss: 0.860788 | Gradient Norm: 0.1869\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1810\u001b[0m\n",
      "\u001b[94mLoss: 0.750658 | Gradient Norm: 0.1920\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1820\u001b[0m\n",
      "\u001b[94mLoss: 0.818140 | Gradient Norm: 0.1437\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1830\u001b[0m\n",
      "\u001b[94mLoss: 1.496520 | Gradient Norm: 0.4241\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1840\u001b[0m\n",
      "\u001b[94mLoss: 0.728873 | Gradient Norm: 0.2312\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1850\u001b[0m\n",
      "\u001b[94mLoss: 1.404373 | Gradient Norm: 0.3798\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1860\u001b[0m\n",
      "\u001b[94mLoss: 1.018573 | Gradient Norm: 0.2782\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1870\u001b[0m\n",
      "\u001b[94mLoss: 0.884048 | Gradient Norm: 0.1498\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1880\u001b[0m\n",
      "\u001b[94mLoss: 0.703045 | Gradient Norm: 0.1777\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1890\u001b[0m\n",
      "\u001b[94mLoss: 0.671903 | Gradient Norm: 0.1552\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1900\u001b[0m\n",
      "\u001b[94mLoss: 0.884428 | Gradient Norm: 0.3307\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1910\u001b[0m\n",
      "\u001b[94mLoss: 0.824725 | Gradient Norm: 0.1524\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1920\u001b[0m\n",
      "\u001b[94mLoss: 0.775545 | Gradient Norm: 0.1352\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 1925.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8576.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8576. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-1925-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1930\u001b[0m\n",
      "\u001b[94mLoss: 0.665662 | Gradient Norm: 0.1809\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1940\u001b[0m\n",
      "\u001b[94mLoss: 0.743188 | Gradient Norm: 0.1489\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1950\u001b[0m\n",
      "\u001b[94mLoss: 0.820219 | Gradient Norm: 0.1264\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1960\u001b[0m\n",
      "\u001b[94mLoss: 0.749116 | Gradient Norm: 0.1366\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1970\u001b[0m\n",
      "\u001b[94mLoss: 0.853908 | Gradient Norm: 0.2417\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1980\u001b[0m\n",
      "\u001b[94mLoss: 0.651152 | Gradient Norm: 0.2322\u001b[0m\n",
      "\u001b[95m\n",
      "Step  1990\u001b[0m\n",
      "\u001b[94mLoss: 0.863038 | Gradient Norm: 0.1129\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2000\u001b[0m\n",
      "\u001b[94mLoss: 0.775725 | Gradient Norm: 0.1376\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2010\u001b[0m\n",
      "\u001b[94mLoss: 0.787756 | Gradient Norm: 0.1716\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2020\u001b[0m\n",
      "\u001b[94mLoss: 0.976819 | Gradient Norm: 0.2425\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2030\u001b[0m\n",
      "\u001b[94mLoss: 0.724633 | Gradient Norm: 0.1311\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2040\u001b[0m\n",
      "\u001b[94mLoss: 0.739254 | Gradient Norm: 0.1656\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2050\u001b[0m\n",
      "\u001b[94mLoss: 0.885655 | Gradient Norm: 0.1600\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2060\u001b[0m\n",
      "\u001b[94mLoss: 1.039898 | Gradient Norm: 0.1893\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2070\u001b[0m\n",
      "\u001b[94mLoss: 0.825614 | Gradient Norm: 0.2102\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2080\u001b[0m\n",
      "\u001b[94mLoss: 0.743223 | Gradient Norm: 0.1168\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2090\u001b[0m\n",
      "\u001b[94mLoss: 0.853913 | Gradient Norm: 0.1478\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2100.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8591.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8576.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2100\u001b[0m\n",
      "\u001b[94mLoss: 0.922167 | Gradient Norm: 0.2738\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2110\u001b[0m\n",
      "\u001b[94mLoss: 0.902099 | Gradient Norm: 0.1515\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2120\u001b[0m\n",
      "\u001b[94mLoss: 1.257429 | Gradient Norm: 0.2884\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2130\u001b[0m\n",
      "\u001b[94mLoss: 0.812144 | Gradient Norm: 0.1355\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2140\u001b[0m\n",
      "\u001b[94mLoss: 0.878973 | Gradient Norm: 0.1334\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2150\u001b[0m\n",
      "\u001b[94mLoss: 0.812123 | Gradient Norm: 0.1369\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2160\u001b[0m\n",
      "\u001b[94mLoss: 0.723372 | Gradient Norm: 0.1365\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2170\u001b[0m\n",
      "\u001b[94mLoss: 0.767636 | Gradient Norm: 0.1391\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2180\u001b[0m\n",
      "\u001b[94mLoss: 0.702300 | Gradient Norm: 0.1107\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2190\u001b[0m\n",
      "\u001b[94mLoss: 0.767861 | Gradient Norm: 0.1231\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2200\u001b[0m\n",
      "\u001b[94mLoss: 0.639128 | Gradient Norm: 0.1265\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2210\u001b[0m\n",
      "\u001b[94mLoss: 0.955529 | Gradient Norm: 0.1287\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2220\u001b[0m\n",
      "\u001b[94mLoss: 1.087860 | Gradient Norm: 0.3711\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2230\u001b[0m\n",
      "\u001b[94mLoss: 0.794808 | Gradient Norm: 0.1030\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2240\u001b[0m\n",
      "\u001b[94mLoss: 0.745019 | Gradient Norm: 0.2317\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2250\u001b[0m\n",
      "\u001b[94mLoss: 0.766222 | Gradient Norm: 0.1519\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2260\u001b[0m\n",
      "\u001b[94mLoss: 0.673739 | Gradient Norm: 0.1861\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2270\u001b[0m\n",
      "\u001b[94mLoss: 0.768026 | Gradient Norm: 0.1943\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2275.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8572.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8572. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-2275-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2280\u001b[0m\n",
      "\u001b[94mLoss: 0.840432 | Gradient Norm: 0.1358\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2290\u001b[0m\n",
      "\u001b[94mLoss: 0.925182 | Gradient Norm: 0.1147\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2300\u001b[0m\n",
      "\u001b[94mLoss: 0.740603 | Gradient Norm: 0.1271\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2310\u001b[0m\n",
      "\u001b[94mLoss: 0.909275 | Gradient Norm: 0.2618\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2320\u001b[0m\n",
      "\u001b[94mLoss: 0.871656 | Gradient Norm: 0.1593\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2330\u001b[0m\n",
      "\u001b[94mLoss: 0.748672 | Gradient Norm: 0.1135\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2340\u001b[0m\n",
      "\u001b[94mLoss: 0.997436 | Gradient Norm: 0.2163\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2350\u001b[0m\n",
      "\u001b[94mLoss: 0.911530 | Gradient Norm: 0.1539\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2360\u001b[0m\n",
      "\u001b[94mLoss: 0.884388 | Gradient Norm: 0.1722\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2370\u001b[0m\n",
      "\u001b[94mLoss: 0.782540 | Gradient Norm: 0.2614\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2380\u001b[0m\n",
      "\u001b[94mLoss: 0.711949 | Gradient Norm: 0.1317\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2390\u001b[0m\n",
      "\u001b[94mLoss: 0.851547 | Gradient Norm: 0.1715\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2400\u001b[0m\n",
      "\u001b[94mLoss: 0.875698 | Gradient Norm: 0.1805\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2410\u001b[0m\n",
      "\u001b[94mLoss: 0.800286 | Gradient Norm: 0.1214\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2420\u001b[0m\n",
      "\u001b[94mLoss: 0.893475 | Gradient Norm: 0.1706\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2430\u001b[0m\n",
      "\u001b[94mLoss: 0.819447 | Gradient Norm: 0.2091\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2440\u001b[0m\n",
      "\u001b[94mLoss: 0.774641 | Gradient Norm: 0.2013\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2450.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8559.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8559. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-2450-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2450\u001b[0m\n",
      "\u001b[94mLoss: 1.062371 | Gradient Norm: 0.1323\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2460\u001b[0m\n",
      "\u001b[94mLoss: 0.904446 | Gradient Norm: 0.2060\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2470\u001b[0m\n",
      "\u001b[94mLoss: 0.820645 | Gradient Norm: 0.1122\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2480\u001b[0m\n",
      "\u001b[94mLoss: 0.852572 | Gradient Norm: 0.1557\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2490\u001b[0m\n",
      "\u001b[94mLoss: 0.928446 | Gradient Norm: 0.2421\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2500\u001b[0m\n",
      "\u001b[94mLoss: 0.863791 | Gradient Norm: 0.2723\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2510\u001b[0m\n",
      "\u001b[94mLoss: 0.617126 | Gradient Norm: 0.2199\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2520\u001b[0m\n",
      "\u001b[94mLoss: 0.797683 | Gradient Norm: 0.1272\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2530\u001b[0m\n",
      "\u001b[94mLoss: 0.687038 | Gradient Norm: 0.2004\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2540\u001b[0m\n",
      "\u001b[94mLoss: 0.851944 | Gradient Norm: 0.2518\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2550\u001b[0m\n",
      "\u001b[94mLoss: 0.795531 | Gradient Norm: 0.2358\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2560\u001b[0m\n",
      "\u001b[94mLoss: 0.752109 | Gradient Norm: 0.1213\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2570\u001b[0m\n",
      "\u001b[94mLoss: 0.921089 | Gradient Norm: 0.2080\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2580\u001b[0m\n",
      "\u001b[94mLoss: 1.010498 | Gradient Norm: 0.2777\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2590\u001b[0m\n",
      "\u001b[94mLoss: 0.821713 | Gradient Norm: 0.1660\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2600\u001b[0m\n",
      "\u001b[94mLoss: 0.711952 | Gradient Norm: 0.2596\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2610\u001b[0m\n",
      "\u001b[94mLoss: 0.822525 | Gradient Norm: 0.1701\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2620\u001b[0m\n",
      "\u001b[94mLoss: 0.868812 | Gradient Norm: 0.2476\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2625.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8556.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8556. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-2625-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2630\u001b[0m\n",
      "\u001b[94mLoss: 0.811536 | Gradient Norm: 0.1060\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2640\u001b[0m\n",
      "\u001b[94mLoss: 0.825008 | Gradient Norm: 0.1569\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2650\u001b[0m\n",
      "\u001b[94mLoss: 0.747161 | Gradient Norm: 0.2284\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2660\u001b[0m\n",
      "\u001b[94mLoss: 0.746298 | Gradient Norm: 0.1178\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2670\u001b[0m\n",
      "\u001b[94mLoss: 0.947679 | Gradient Norm: 0.2026\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2680\u001b[0m\n",
      "\u001b[94mLoss: 0.745872 | Gradient Norm: 0.1336\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2690\u001b[0m\n",
      "\u001b[94mLoss: 0.813754 | Gradient Norm: 0.1318\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2700\u001b[0m\n",
      "\u001b[94mLoss: 0.735201 | Gradient Norm: 0.1584\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2710\u001b[0m\n",
      "\u001b[94mLoss: 0.796043 | Gradient Norm: 0.1832\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2720\u001b[0m\n",
      "\u001b[94mLoss: 0.713440 | Gradient Norm: 0.1214\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2730\u001b[0m\n",
      "\u001b[94mLoss: 0.817902 | Gradient Norm: 0.1318\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2740\u001b[0m\n",
      "\u001b[94mLoss: 1.112567 | Gradient Norm: 0.2211\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2750\u001b[0m\n",
      "\u001b[94mLoss: 0.870771 | Gradient Norm: 0.1966\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2760\u001b[0m\n",
      "\u001b[94mLoss: 0.811543 | Gradient Norm: 0.2573\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2770\u001b[0m\n",
      "\u001b[94mLoss: 0.814032 | Gradient Norm: 0.1620\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2780\u001b[0m\n",
      "\u001b[94mLoss: 0.985791 | Gradient Norm: 0.1960\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2790\u001b[0m\n",
      "\u001b[94mLoss: 1.206038 | Gradient Norm: 0.2630\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2800.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8561.\u001b[0m\n",
      "\u001b[93mNo improvement in validation loss. Current best: 0.8556.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2800\u001b[0m\n",
      "\u001b[94mLoss: 0.739777 | Gradient Norm: 0.2804\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 2810.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.8533.\u001b[0m\n",
      "\u001b[92mNew best validation loss: 0.8533. Model checkpoint saved at checkpoints\\sssm-Walker2D-v1-model_step-2810-2024-11-18-22-01-56.pt.\u001b[0m\n",
      "\u001b[95m\n",
      "Step  2810\u001b[0m\n",
      "\u001b[94mLoss: 0.739908 | Gradient Norm: 0.1221\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "for epoch in range(num_epochs):\n",
    "    for step, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs,  targets = inputs.to(device), targets.to(device)\n",
    "        relative_step = epoch * steps_per_epoch + step\n",
    "        last_step = relative_step == num_steps - 1\n",
    "        # print(inputs.shape,  targets.shape)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss,  metrics = loss_fn(outputs, targets)  # Assuming `loss_function` is defined\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip global norm of gradient at 1.0, per the GPT-3 paper\n",
    "        grad_norms.append(grad_norm.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        # Periodically evaluate the model on validation set\n",
    "        if relative_step % (eval_period // dilation) == 0 or last_step:\n",
    "            colored_print(\n",
    "                f\"\\nEvaluating the spectral SSM model at step {relative_step}.\",\n",
    "                Colors.OKCYAN,\n",
    "            )\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss, metrics = loss_fn(val_outputs, val_targets) \n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= len(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                val_time_steps.append(relative_step)\n",
    "            model.train()\n",
    "\n",
    "            colored_print(\n",
    "                f\"\\nValidation Loss: {val_loss:.4f}.\",\n",
    "                Colors.OKCYAN,\n",
    "            )\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_step = relative_step\n",
    "                patient_counter = 0\n",
    "\n",
    "                # Save model and optimizer state\n",
    "                model_checkpoint = f\"sssm-{controller}-model_step-{relative_step}-{timestamp}.pt\"\n",
    "               \n",
    "                model_path = os.path.join(checkpoint_dir, model_checkpoint)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "                extra_info = f\"sssm-{controller}-other_step-{relative_step}-{timestamp}.pt\"\n",
    "                best_checkpoint =model_checkpoint, extra_info\n",
    "                extra_info_path = os.path.join(checkpoint_dir, extra_info)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"configs\": model.configs,\n",
    "                        \"step\": relative_step,\n",
    "                        \"val_loss\": val_loss,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"rng_state_pytorch\": torch.get_rng_state(),\n",
    "                        \"rng_state_numpy\": np.random.get_state(),\n",
    "                        \"rng_state_cuda\": torch.cuda.get_rng_state_all()\n",
    "                        if torch.cuda.is_available()\n",
    "                        else None,\n",
    "                    },\n",
    "                    extra_info_path,\n",
    "                )\n",
    "                colored_print(\n",
    "                    f\"New best validation loss: {val_loss:.4f}. Model checkpoint saved at {model_path}.\",\n",
    "                    Colors.OKGREEN,\n",
    "                )\n",
    "            else:\n",
    "                patient_counter += 1\n",
    "                colored_print(\n",
    "                    f\"No improvement in validation loss. Current best: {best_val_loss:.4f}.\",\n",
    "                    Colors.WARNING,\n",
    "                )\n",
    "\n",
    "            if patient_counter >= patience:\n",
    "                colored_print(\n",
    "                    f\"Stopping early due to patience limit of {patience}.\",\n",
    "                    Colors.FAIL,\n",
    "                )\n",
    "                break\n",
    "\n",
    "        # Logging\n",
    "        if relative_step % 10 == 0:\n",
    "            colored_print(f\"\\nStep {relative_step:5d}\", Colors.HEADER)\n",
    "            colored_print(f\"Loss: {loss.item():.6f} | Gradient Norm: {grad_norm:.4f}\", Colors.OKBLUE)\n",
    "\n",
    "    if patient_counter >= patience:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model information for the spectral SSM model:\n",
      "    Best model at step 2810\n",
      "    Best validation loss: 0.8533\n",
      "    Best model checkpoint saved at: checkpoints\\sssm-Walker2D-v1-model_step-2810-2024-11-18-22-01-56.pt\n",
      "    Additional data saved at: checkpoints\\sssm-Walker2D-v1-other_step-2810-2024-11-18-22-01-56.pt\n",
      "Training details saved in training_details_sssm_2024-11-18-22-01-56.txt. Congratulations!\n",
      "Data saved to results\\mujoco-v1\\sssm\\sssm-Walker2D-v1-train_losses-2024-11-18-22-01-56.txt\n",
      "Data saved to results\\mujoco-v1\\sssm\\sssm-Walker2D-v1-val_losses-2024-11-18-22-01-56.txt\n",
      "Data saved to results\\mujoco-v1\\sssm\\sssm-Walker2D-v1-val_time_steps-2024-11-18-22-01-56.txt\n",
      "Data saved to results\\mujoco-v1\\sssm\\sssm-Walker2D-v1-grad_norms-2024-11-18-22-01-56.txt\n",
      "Training run completed. Results have been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_17000\\3573879908.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n",
      "C:\\Users\\devan\\AppData\\Local\\Temp\\ipykernel_17000\\3573879908.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  other_data = torch.load(best_model_extra_info_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "from  train import save_results\n",
    "task = 'mujoco-v1'\n",
    "\n",
    "if best_checkpoint:\n",
    "    model_checkpoint, extra_info = best_checkpoint\n",
    "    best_model_path = os.path.join(checkpoint_dir, model_checkpoint)\n",
    "    best_model_extra_info_path = os.path.join(checkpoint_dir, extra_info)\n",
    "\n",
    "    # Load the best checkpoint\n",
    "    state_dict = torch.load(best_model_path, map_location=\"cuda\")\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Load optimizer and other data\n",
    "    other_data = torch.load(best_model_extra_info_path, map_location=\"cpu\")\n",
    "    optimizer.load_state_dict(other_data[\"optimizer\"])\n",
    "\n",
    "    # Display best model information\n",
    "    print(\"\\nBest model information for the spectral SSM model:\")\n",
    "    print(f\"    Best model at step {best_model_step}\")\n",
    "    print(f\"    Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"    Best model checkpoint saved at: {best_model_path}\")\n",
    "    print(f\"    Additional data saved at: {best_model_extra_info_path}\")\n",
    "\n",
    "    # Save training details to a file\n",
    "    training_details_path = f\"training_details_sssm_{timestamp}.txt\"\n",
    "    with open(training_details_path, \"w\") as f:\n",
    "        f.write(f\"Training completed for the spectral SSM on {task} at: {datetime.now()}\\n\")\n",
    "        f.write(f\"Best model step: {best_model_step}\\n\")\n",
    "        f.write(f\"Best validation loss: {best_val_loss:.4f}\\n\")\n",
    "        f.write(f\"Best model checkpoint saved at: {best_model_path}\\n\")\n",
    "        f.write(f\"Additional data saved at: {best_model_extra_info_path}\\n\")\n",
    "\n",
    "    print(f\"Training details saved in {training_details_path}. Congratulations!\")\n",
    "else:\n",
    "    print(\"\\nNo best checkpoint found. The model did not improve during training.\")\n",
    "\n",
    "# Save final results\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "if not os.path.exists(\"results/\"):\n",
    "    os.makedirs(\"results/\")\n",
    "if not os.path.exists(\"landscapes\"):\n",
    "    os.makedirs(\"landscapes\", exist_ok=True)\n",
    "\n",
    "save_results(task, controller, train_losses, \"train_losses\", timestamp)\n",
    "save_results(task, controller,val_losses, \"val_losses\", timestamp)\n",
    "save_results(task, controller, val_time_steps,\"val_time_steps\", timestamp)\n",
    "save_results(task, controller, grad_norms, \"grad_norms\", timestamp)\n",
    "\n",
    "print(\"Training run completed. Results have been saved.\")\n",
    "\n",
    "# from utils.loss_landscape import LossLandscape (takes 20 minutes to run)\n",
    "# #Don't exactly know how to visualize thi\n",
    "# loss_landscape = LossLandscape(model, device, optimizer, lr)\n",
    "# x_range = (-1, 1, 10)  # Adjust as needed\n",
    "# y_range = (-1, 1, 10)\n",
    "# loss_landscape.generate(\n",
    "#     train_loader,\n",
    "#     f\"landscapes/loss_landscape-{timestamp}\",\n",
    "#     x_range=x_range,\n",
    "#     y_range=y_range,\n",
    "#     plot_loss_landscape=True,\n",
    "#     plot_hessian=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSSM(\n",
       "  (loss_fn): MSELoss()\n",
       "  (emb): Linear(in_features=24, out_features=8, bias=False)\n",
       "  (stu): ModuleDict(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (hidden): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (rn_1): RMSNorm((8,), eps=None, elementwise_affine=True)\n",
       "        (stu): STU(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (rn_2): RMSNorm((8,), eps=None, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (swiglu): SwiGLU(\n",
       "            (w): Linear(in_features=8, out_features=32, bias=False)\n",
       "            (v): Linear(in_features=8, out_features=32, bias=False)\n",
       "            (w2): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (task_head): Linear(in_features=8, out_features=18, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  copy\n",
    "stu_layers = [copy.deepcopy(block.stu) for block in model.stu['hidden']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STU(\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDS(nn.Module):\n",
    "    def __init__(self, state_dim, input_dim, output_dim):\n",
    "        super(LDS, self).__init__()\n",
    "        \n",
    "        self.d_out = output_dim\n",
    "\n",
    "        self.h0 = nn.Parameter(torch.randn(state_dim))\n",
    "        # self.A = nn.Parameter(torch.clip(torch.randn(state_dim), -.7, 0.7))\n",
    "        init_A = torch.randn(state_dim)\n",
    "        self.A = nn.Parameter(init_A/torch.max(torch.abs(init_A)))\n",
    "        self.B = nn.Parameter(torch.randn(input_dim, state_dim) / input_dim)\n",
    "        self.C = nn.Parameter(torch.randn(state_dim,output_dim) / state_dim)\n",
    "        self.D = nn.Parameter(torch.randn(input_dim,output_dim) / output_dim) #keep for more complex systems\n",
    "\n",
    "        self.M = nn.Parameter(torch.randn(output_dim, output_dim) / output_dim) #autoregressive\n",
    "    def forward(self, inputs):\n",
    "        bsz, seq_len, _ = inputs.shape\n",
    "        h_t = self.h0.expand(bsz, self.h0.shape[0]).to(device)  # Ensure h0 is on the correct device\n",
    "        outputs = []\n",
    "        A = self.A.flatten()\n",
    "\n",
    "        # Store all intermediate h_t states\n",
    "        all_h_t = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            u_t = inputs[:, t, :]  # Get input for all batches at time t\n",
    "            h_t = A * h_t + u_t @ self.B  # Update hidden states for all batches\n",
    "            all_h_t.append(h_t.unsqueeze(1))  # Store the updated h_t for each time step\n",
    "\n",
    "        # Concatenate all h_t states along the time dimension\n",
    "        all_h_t = torch.cat(all_h_t, dim=1)\n",
    "\n",
    "        # Apply C to all concatenated h_t states at once\n",
    "        outputs = torch.matmul(all_h_t, self.C)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def compute_loss(self, inputs, targets):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        outputs = self(inputs)\n",
    "        return mse_loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GeneralLDS(nn.Module):\n",
    "    def __init__(self, state_dim, input_dim, output_dim):\n",
    "        super(GeneralLDS, self).__init__()\n",
    "        \n",
    "        self.d_out = output_dim\n",
    "\n",
    "        self.h0 = nn.Parameter(torch.randn(state_dim))\n",
    "        # Initialize A as a full matrix\n",
    "        init_A = torch.randn(state_dim, state_dim) / input_dim \n",
    "        max_evec = torch.max(torch.abs(torch.linalg.eigvals(init_A)))\n",
    "        self.A = nn.Parameter(init_A / (max_evec * state_dim))\n",
    "        self.B = nn.Parameter(torch.randn(input_dim, state_dim) / input_dim)\n",
    "        self.C = nn.Parameter(torch.randn(state_dim, output_dim) / state_dim)\n",
    "        self.D = nn.Parameter(torch.randn(input_dim, output_dim) / output_dim) # Optional for more complex systems\n",
    "\n",
    "        self.M = nn.Parameter(torch.randn(output_dim, output_dim) / output_dim) # Optional autoregressive component\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        bsz, seq_len, _ = inputs.shape\n",
    "        h_t = self.h0.expand(bsz, -1).to(inputs.device)  # Ensure h0 is on the correct device\n",
    "        outputs = []\n",
    "        \n",
    "        # Process each time step\n",
    "        for t in range(seq_len):\n",
    "            u_t = inputs[:, t, :]  # Get input for all batches at time t\n",
    "            h_t = torch.matmul(h_t, self.A) + torch.matmul(u_t, self.B)  # Update hidden states using full matrix A\n",
    "            outputs.append(torch.matmul(h_t, self.C).unsqueeze(1))  # Calculate output and store\n",
    "\n",
    "        # Concatenate outputs along the time dimension\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_loss(self, inputs, targets):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        outputs = self(inputs)\n",
    "        return mse_loss(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_LDS(nn.Module):\n",
    "    def __init__(self, state_dim, input_dim, output_dim):\n",
    "        super(SVD_LDS, self).__init__()\n",
    "        \n",
    "        self.d_out = output_dim\n",
    "\n",
    "        self.h0 = nn.Parameter(torch.randn(state_dim))\n",
    "        \n",
    "        # Initialize A as a full matrix and perform SVD\n",
    "        init_A = torch.randn(state_dim, state_dim) / input_dim \n",
    "        U, S, V = torch.linalg.svd(init_A, full_matrices=False)\n",
    "        S_clamped = torch.clamp(S, max=1.0)  # Clamp the singular values\n",
    "\n",
    "        # Store SVD components as parameters\n",
    "        self.U = nn.Parameter(U)\n",
    "        self.S = nn.Parameter(S_clamped)\n",
    "        self.V = nn.Parameter(V)\n",
    "\n",
    "        self.B = nn.Parameter(torch.randn(input_dim, state_dim) / input_dim)\n",
    "        self.C = nn.Parameter(torch.randn(state_dim, output_dim) / state_dim)\n",
    "        self.D = nn.Parameter(torch.randn(input_dim, output_dim) / output_dim)  # Optional\n",
    "\n",
    "        self.M = nn.Parameter(torch.randn(output_dim, output_dim) / output_dim)  # Optional\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        bsz, seq_len, _ = inputs.shape\n",
    "        h_t = self.h0.expand(bsz, -1).to(inputs.device)  # Ensure h0 is on the correct device\n",
    "        outputs = []\n",
    "        \n",
    "        # Normalize columns of U and rows of V\n",
    "        U_norm = torch.nn.functional.normalize(self.U, p=2, dim=0)\n",
    "        V_norm = torch.nn.functional.normalize(self.V, p=2, dim=1)\n",
    "        \n",
    "        # Reconstruct A using normalized U, S, and V\n",
    "        A = U_norm @ torch.diag(self.S) @ V_norm.t()\n",
    "\n",
    "        # Process each time step\n",
    "        for t in range(seq_len):\n",
    "            u_t = inputs[:, t, :]  # Get input for all batches at time t\n",
    "            h_t = torch.matmul(h_t, A) + torch.matmul(u_t, self.B)  # Update hidden states using A\n",
    "            outputs.append(torch.matmul(h_t, self.C).unsqueeze(1))  # Calculate output and store\n",
    "\n",
    "        # Concatenate outputs along the time dimension\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_loss(self, inputs, targets):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        outputs = self(inputs)\n",
    "        return mse_loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 8])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(1, sl, n_embd).to(device)\n",
    "stu_layers[0](inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSSM(\n",
       "  (loss_fn): MSELoss()\n",
       "  (emb): Linear(in_features=24, out_features=8, bias=False)\n",
       "  (stu): ModuleDict(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (hidden): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (rn_1): RMSNorm((8,), eps=None, elementwise_affine=True)\n",
       "        (stu): STU(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (rn_2): RMSNorm((8,), eps=None, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (swiglu): SwiGLU(\n",
       "            (w): Linear(in_features=8, out_features=32, bias=False)\n",
       "            (v): Linear(in_features=8, out_features=32, bias=False)\n",
       "            (w2): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (task_head): Linear(in_features=8, out_features=18, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.475245475769043\n",
      "Epoch 100, Loss: 1.2763808965682983\n",
      "Epoch 200, Loss: 1.0696425437927246\n",
      "Epoch 300, Loss: 0.8730352520942688\n",
      "Epoch 400, Loss: 0.6907178163528442\n",
      "Epoch 500, Loss: 0.5526415109634399\n",
      "Epoch 600, Loss: 0.45476576685905457\n",
      "Epoch 700, Loss: 0.38495343923568726\n",
      "Epoch 800, Loss: 0.3613731265068054\n",
      "Epoch 900, Loss: 0.3310360908508301\n",
      "Epoch 1000, Loss: 0.2923634648323059\n"
     ]
    }
   ],
   "source": [
    "state_dim = 100 #@param\n",
    "lds =  LDS(state_dim, n_embd, n_embd).to(device)\n",
    "optimizer = torch.optim.Adam(lds.parameters(), lr = 0.0001)\n",
    "lds_epochs = 5000\n",
    "lds_loss_values = []\n",
    "\n",
    "for epoch in range(lds_epochs):\n",
    "    inputs = torch.randn(1, sl, n_embd).to(device)\n",
    "    stu_outputs = stu_layers[0](inputs).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = lds.compute_loss(inputs, stu_outputs)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lds.parameters(), max_norm=1)\n",
    "    lds_loss_values.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lds.A.data.clamp_(max=1, min = -1)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.7042, device='cuda:0'),\n",
       " tensor(-0.5758, device='cuda:0'),\n",
       " tensor(-0.5220, device='cuda:0'),\n",
       " tensor(-0.5192, device='cuda:0'),\n",
       " tensor(-0.4873, device='cuda:0'),\n",
       " tensor(-0.4677, device='cuda:0'),\n",
       " tensor(-0.4620, device='cuda:0'),\n",
       " tensor(-0.4501, device='cuda:0'),\n",
       " tensor(-0.4488, device='cuda:0'),\n",
       " tensor(-0.4475, device='cuda:0'),\n",
       " tensor(-0.4342, device='cuda:0'),\n",
       " tensor(-0.3812, device='cuda:0'),\n",
       " tensor(-0.3699, device='cuda:0'),\n",
       " tensor(-0.2248, device='cuda:0'),\n",
       " tensor(-0.1654, device='cuda:0'),\n",
       " tensor(-0.1580, device='cuda:0'),\n",
       " tensor(-0.1517, device='cuda:0'),\n",
       " tensor(-0.1469, device='cuda:0'),\n",
       " tensor(-0.1281, device='cuda:0'),\n",
       " tensor(-0.1011, device='cuda:0'),\n",
       " tensor(-0.0902, device='cuda:0'),\n",
       " tensor(-0.0816, device='cuda:0'),\n",
       " tensor(-0.0814, device='cuda:0'),\n",
       " tensor(-0.0810, device='cuda:0'),\n",
       " tensor(-0.0760, device='cuda:0'),\n",
       " tensor(-0.0561, device='cuda:0'),\n",
       " tensor(-0.0403, device='cuda:0'),\n",
       " tensor(-0.0395, device='cuda:0'),\n",
       " tensor(-0.0270, device='cuda:0'),\n",
       " tensor(-0.0168, device='cuda:0'),\n",
       " tensor(-0.0024, device='cuda:0'),\n",
       " tensor(0.0344, device='cuda:0'),\n",
       " tensor(0.0401, device='cuda:0'),\n",
       " tensor(0.0446, device='cuda:0'),\n",
       " tensor(0.0566, device='cuda:0'),\n",
       " tensor(0.0637, device='cuda:0'),\n",
       " tensor(0.0651, device='cuda:0'),\n",
       " tensor(0.0674, device='cuda:0'),\n",
       " tensor(0.0709, device='cuda:0'),\n",
       " tensor(0.0739, device='cuda:0'),\n",
       " tensor(0.0756, device='cuda:0'),\n",
       " tensor(0.0769, device='cuda:0'),\n",
       " tensor(0.0796, device='cuda:0'),\n",
       " tensor(0.0821, device='cuda:0'),\n",
       " tensor(0.0859, device='cuda:0'),\n",
       " tensor(0.1021, device='cuda:0'),\n",
       " tensor(0.1083, device='cuda:0'),\n",
       " tensor(0.1224, device='cuda:0'),\n",
       " tensor(0.1322, device='cuda:0'),\n",
       " tensor(0.1338, device='cuda:0'),\n",
       " tensor(0.1406, device='cuda:0'),\n",
       " tensor(0.1639, device='cuda:0'),\n",
       " tensor(0.1643, device='cuda:0'),\n",
       " tensor(0.1673, device='cuda:0'),\n",
       " tensor(0.1756, device='cuda:0'),\n",
       " tensor(0.1777, device='cuda:0'),\n",
       " tensor(0.1916, device='cuda:0'),\n",
       " tensor(0.1996, device='cuda:0'),\n",
       " tensor(0.2140, device='cuda:0'),\n",
       " tensor(0.2275, device='cuda:0'),\n",
       " tensor(0.2339, device='cuda:0'),\n",
       " tensor(0.2351, device='cuda:0'),\n",
       " tensor(0.2393, device='cuda:0'),\n",
       " tensor(0.2473, device='cuda:0'),\n",
       " tensor(0.2621, device='cuda:0'),\n",
       " tensor(0.2655, device='cuda:0'),\n",
       " tensor(0.2660, device='cuda:0'),\n",
       " tensor(0.2672, device='cuda:0'),\n",
       " tensor(0.2726, device='cuda:0'),\n",
       " tensor(0.2887, device='cuda:0'),\n",
       " tensor(0.2910, device='cuda:0'),\n",
       " tensor(0.3472, device='cuda:0'),\n",
       " tensor(0.3604, device='cuda:0'),\n",
       " tensor(0.3744, device='cuda:0'),\n",
       " tensor(0.3815, device='cuda:0'),\n",
       " tensor(0.3879, device='cuda:0'),\n",
       " tensor(0.4029, device='cuda:0'),\n",
       " tensor(0.4074, device='cuda:0'),\n",
       " tensor(0.4075, device='cuda:0'),\n",
       " tensor(0.4114, device='cuda:0'),\n",
       " tensor(0.4131, device='cuda:0'),\n",
       " tensor(0.4362, device='cuda:0'),\n",
       " tensor(0.4369, device='cuda:0'),\n",
       " tensor(0.4505, device='cuda:0'),\n",
       " tensor(0.4680, device='cuda:0'),\n",
       " tensor(0.4848, device='cuda:0'),\n",
       " tensor(0.4859, device='cuda:0'),\n",
       " tensor(0.4936, device='cuda:0'),\n",
       " tensor(0.5088, device='cuda:0'),\n",
       " tensor(0.5131, device='cuda:0'),\n",
       " tensor(0.5384, device='cuda:0'),\n",
       " tensor(0.5409, device='cuda:0'),\n",
       " tensor(0.5994, device='cuda:0'),\n",
       " tensor(0.6119, device='cuda:0'),\n",
       " tensor(0.6624, device='cuda:0'),\n",
       " tensor(0.6981, device='cuda:0'),\n",
       " tensor(0.7190, device='cuda:0'),\n",
       " tensor(0.8082, device='cuda:0'),\n",
       " tensor(0.8836, device='cuda:0'),\n",
       " tensor(0.8958, device='cuda:0')]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lds.A.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import my code over for visualizing the STU kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c00dfb8fa0>]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lds_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3077785968780518\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m lds2\u001b[38;5;241m.\u001b[39mcompute_loss(inputs, stu_outputs)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(lds2\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m lds_loss_values2\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_dim = 100 #@param\n",
    "lds2 =  LDS(state_dim, n_embd, n_embd).to(device)\n",
    "optimizer = torch.optim.Adam(lds2.parameters(), lr = 0.0001)\n",
    "lds_epochs = 5000\n",
    "lds_loss_values2 = []\n",
    "\n",
    "for epoch in range(lds_epochs):\n",
    "    inputs = torch.randn(1, sl, n_embd).to(device)\n",
    "    stu_outputs = stu_layers[1](inputs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = lds2.compute_loss(inputs, stu_outputs)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lds2.parameters(), max_norm=1)\n",
    "    lds_loss_values2.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lds2.A.data.clamp_(max=1, min = -1)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c00dfc00d0>]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lds_loss_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSSM(\n",
       "  (loss_fn): MSELoss()\n",
       "  (emb): Linear(in_features=24, out_features=8, bias=False)\n",
       "  (stu): ModuleDict(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (hidden): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (rn_1): RMSNorm((8,), eps=None, elementwise_affine=True)\n",
       "        (stu): LDS()\n",
       "        (rn_2): RMSNorm((8,), eps=None, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (swiglu): SwiGLU(\n",
       "            (w): Linear(in_features=8, out_features=32, bias=False)\n",
       "            (v): Linear(in_features=8, out_features=32, bias=False)\n",
       "            (w2): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (task_head): Linear(in_features=8, out_features=18, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = copy.deepcopy(model)\n",
    "\n",
    "model2.stu['hidden'][0].stu = lds\n",
    "model2.stu['hidden'][1].stu = lds2\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 703/703 [01:35<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "def evaluate_model(model, val_loader, loss_fn, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Ensure no gradients are calculated during validation\n",
    "        for val_inputs, val_targets in tqdm.tqdm(val_loader):\n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            loss, _ = loss_fn(val_outputs, val_targets)  # Assume metrics return is not needed here\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)  # Average the validation loss\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "validation_loss = evaluate_model(model2, val_loader, loss_fn, device)\n",
    "print(f\"Validation Loss: {validation_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adagrad(model2.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 0.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.9619.\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 5.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.9519.\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 10.\u001b[0m\n",
      "\u001b[96m\n",
      "Validation Loss: 0.9473.\u001b[0m\n",
      "\u001b[96m\n",
      "Evaluating the spectral SSM model at step 15.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [05:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_inputs, val_targets \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     33\u001b[0m     val_inputs, val_targets \u001b[38;5;241m=\u001b[39m val_inputs\u001b[38;5;241m.\u001b[39mto(device), val_targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 34\u001b[0m     val_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     loss, metrics \u001b[38;5;241m=\u001b[39m loss_fn(val_outputs, val_targets) \n\u001b[0;32m     36\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[47], line 215\u001b[0m, in \u001b[0;36mSSSM.forward\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m    212\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstu\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstu\u001b[38;5;241m.\u001b[39mhidden:\n\u001b[1;32m--> 215\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_head(x)\n\u001b[0;32m    219\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(preds, targets) \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[47], line 147\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03mForward pass of the Block.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output tensor\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Good configuration\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrn_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrn_2(x))\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Basic configuration (no skips, no non-linearities)\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# x = self.stu(x)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[166], line 21\u001b[0m, in \u001b[0;36mLDS.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     19\u001b[0m     u_t \u001b[38;5;241m=\u001b[39m inputs[:, t, :]  \u001b[38;5;66;03m# Get input for all batches at time t\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     h_t \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m*\u001b[39m h_t \u001b[38;5;241m+\u001b[39m u_t \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB  \u001b[38;5;66;03m# Update hidden states for all batches\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     y_t \u001b[38;5;241m=\u001b[39m h_t \u001b[38;5;241m@\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m  \u001b[38;5;66;03m# Compute output for all batches\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(y_t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Unsqueeze to preserve the sequence dimension\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\devan\\anaconda3\\envs\\sssm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1920\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.train()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for step, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs,  targets = inputs.to(device), targets.to(device)\n",
    "        relative_step = epoch * steps_per_epoch + step\n",
    "        last_step = relative_step == num_steps - 1\n",
    "        # print(inputs.shape,  targets.shape)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model2(inputs)\n",
    "        loss,  metrics = loss_fn(outputs, targets)  # Assuming `loss_function` is defined\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model2.parameters(), 1.0)  # Clip global norm of gradient at 1.0, per the GPT-3 paper\n",
    "        grad_norms.append(grad_norm.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        # Periodically evaluate the model on validation set\n",
    "        if relative_step % 5 == 0 or last_step:\n",
    "            colored_print(\n",
    "                f\"\\nEvaluating the spectral SSM model at step {relative_step}.\",\n",
    "                Colors.OKCYAN,\n",
    "            )\n",
    "            model2.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_outputs = model2(val_inputs)\n",
    "                    loss, metrics = loss_fn(val_outputs, val_targets) \n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= len(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                val_time_steps.append(relative_step)\n",
    "            model2.train()\n",
    "\n",
    "            colored_print(\n",
    "                f\"\\nValidation Loss: {val_loss:.4f}.\",\n",
    "                Colors.OKCYAN,\n",
    "            )\n",
    "\n",
    "    if patient_counter >= patience:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    torch.save(model2, \"LDS_replaced_SSM_model.pth\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = copy.deepcopy(model)\n",
    "\n",
    "model3.stu['hidden'][0].stu = LDS(20, n_embd, n_embd)\n",
    "model3.stu['hidden'][1].stu = LDS(20, n_embd, n_embd)\n",
    "model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.7042, device='cuda:0'),\n",
       " tensor(-0.5758, device='cuda:0'),\n",
       " tensor(-0.5220, device='cuda:0'),\n",
       " tensor(-0.5192, device='cuda:0'),\n",
       " tensor(-0.4873, device='cuda:0'),\n",
       " tensor(-0.4677, device='cuda:0'),\n",
       " tensor(-0.4620, device='cuda:0'),\n",
       " tensor(-0.4501, device='cuda:0'),\n",
       " tensor(-0.4488, device='cuda:0'),\n",
       " tensor(-0.4475, device='cuda:0'),\n",
       " tensor(-0.4342, device='cuda:0'),\n",
       " tensor(-0.3812, device='cuda:0'),\n",
       " tensor(-0.3699, device='cuda:0'),\n",
       " tensor(-0.2248, device='cuda:0'),\n",
       " tensor(-0.1654, device='cuda:0'),\n",
       " tensor(-0.1580, device='cuda:0'),\n",
       " tensor(-0.1517, device='cuda:0'),\n",
       " tensor(-0.1469, device='cuda:0'),\n",
       " tensor(-0.1281, device='cuda:0'),\n",
       " tensor(-0.1011, device='cuda:0'),\n",
       " tensor(-0.0902, device='cuda:0'),\n",
       " tensor(-0.0816, device='cuda:0'),\n",
       " tensor(-0.0814, device='cuda:0'),\n",
       " tensor(-0.0810, device='cuda:0'),\n",
       " tensor(-0.0760, device='cuda:0'),\n",
       " tensor(-0.0561, device='cuda:0'),\n",
       " tensor(-0.0403, device='cuda:0'),\n",
       " tensor(-0.0395, device='cuda:0'),\n",
       " tensor(-0.0270, device='cuda:0'),\n",
       " tensor(-0.0168, device='cuda:0'),\n",
       " tensor(-0.0024, device='cuda:0'),\n",
       " tensor(0.0344, device='cuda:0'),\n",
       " tensor(0.0401, device='cuda:0'),\n",
       " tensor(0.0446, device='cuda:0'),\n",
       " tensor(0.0566, device='cuda:0'),\n",
       " tensor(0.0637, device='cuda:0'),\n",
       " tensor(0.0651, device='cuda:0'),\n",
       " tensor(0.0674, device='cuda:0'),\n",
       " tensor(0.0709, device='cuda:0'),\n",
       " tensor(0.0739, device='cuda:0'),\n",
       " tensor(0.0756, device='cuda:0'),\n",
       " tensor(0.0769, device='cuda:0'),\n",
       " tensor(0.0796, device='cuda:0'),\n",
       " tensor(0.0821, device='cuda:0'),\n",
       " tensor(0.0859, device='cuda:0'),\n",
       " tensor(0.1021, device='cuda:0'),\n",
       " tensor(0.1083, device='cuda:0'),\n",
       " tensor(0.1224, device='cuda:0'),\n",
       " tensor(0.1322, device='cuda:0'),\n",
       " tensor(0.1338, device='cuda:0'),\n",
       " tensor(0.1406, device='cuda:0'),\n",
       " tensor(0.1639, device='cuda:0'),\n",
       " tensor(0.1643, device='cuda:0'),\n",
       " tensor(0.1673, device='cuda:0'),\n",
       " tensor(0.1756, device='cuda:0'),\n",
       " tensor(0.1777, device='cuda:0'),\n",
       " tensor(0.1916, device='cuda:0'),\n",
       " tensor(0.1996, device='cuda:0'),\n",
       " tensor(0.2140, device='cuda:0'),\n",
       " tensor(0.2275, device='cuda:0'),\n",
       " tensor(0.2339, device='cuda:0'),\n",
       " tensor(0.2351, device='cuda:0'),\n",
       " tensor(0.2393, device='cuda:0'),\n",
       " tensor(0.2473, device='cuda:0'),\n",
       " tensor(0.2621, device='cuda:0'),\n",
       " tensor(0.2655, device='cuda:0'),\n",
       " tensor(0.2660, device='cuda:0'),\n",
       " tensor(0.2672, device='cuda:0'),\n",
       " tensor(0.2726, device='cuda:0'),\n",
       " tensor(0.2887, device='cuda:0'),\n",
       " tensor(0.2910, device='cuda:0'),\n",
       " tensor(0.3472, device='cuda:0'),\n",
       " tensor(0.3604, device='cuda:0'),\n",
       " tensor(0.3744, device='cuda:0'),\n",
       " tensor(0.3815, device='cuda:0'),\n",
       " tensor(0.3879, device='cuda:0'),\n",
       " tensor(0.4029, device='cuda:0'),\n",
       " tensor(0.4074, device='cuda:0'),\n",
       " tensor(0.4075, device='cuda:0'),\n",
       " tensor(0.4114, device='cuda:0'),\n",
       " tensor(0.4131, device='cuda:0'),\n",
       " tensor(0.4362, device='cuda:0'),\n",
       " tensor(0.4369, device='cuda:0'),\n",
       " tensor(0.4505, device='cuda:0'),\n",
       " tensor(0.4680, device='cuda:0'),\n",
       " tensor(0.4848, device='cuda:0'),\n",
       " tensor(0.4859, device='cuda:0'),\n",
       " tensor(0.4936, device='cuda:0'),\n",
       " tensor(0.5088, device='cuda:0'),\n",
       " tensor(0.5131, device='cuda:0'),\n",
       " tensor(0.5384, device='cuda:0'),\n",
       " tensor(0.5409, device='cuda:0'),\n",
       " tensor(0.5994, device='cuda:0'),\n",
       " tensor(0.6119, device='cuda:0'),\n",
       " tensor(0.6624, device='cuda:0'),\n",
       " tensor(0.6981, device='cuda:0'),\n",
       " tensor(0.7190, device='cuda:0'),\n",
       " tensor(0.8082, device='cuda:0'),\n",
       " tensor(0.8836, device='cuda:0'),\n",
       " tensor(0.8958, device='cuda:0')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adagrad(model3.parameters(), lr=0.0001)\n",
    "model3.train()\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for step, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs,  targets = inputs.to(device), targets.to(device)\n",
    "        relative_step = epoch * steps_per_epoch + step\n",
    "        last_step = relative_step == num_steps - 1\n",
    "        # print(inputs.shape,  targets.shape)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model3(inputs)\n",
    "        loss,  metrics = loss_fn(outputs, targets)  # Assuming `loss_function` is defined\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model3.parameters(), 1.0)  # Clip global norm of gradient at 1.0, per the GPT-3 paper\n",
    "        grad_norms.append(grad_norm.item())\n",
    "        optimizer.step()\n",
    "\n",
    "        # Periodically evaluate the model on validation set\n",
    "        if relative_step % 5 == 0 or last_step:\n",
    "            colored_print(\n",
    "                f\"\\nEvaluating the spectral SSM model at step {relative_step}.\",\n",
    "                Colors.OKCYAN,\n",
    "            )\n",
    "            model3.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0.0\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_outputs = model3(val_inputs)\n",
    "                    loss, metrics = loss_fn(val_outputs, val_targets) \n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= len(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                val_time_steps.append(relative_step)\n",
    "            model3.train()\n",
    "\n",
    "            colored_print(\n",
    "                f\"\\nValidation Loss: {val_loss:.4f}.\",\n",
    "                Colors.OKCYAN,\n",
    "            )\n",
    "\n",
    "    if patient_counter >= patience:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
